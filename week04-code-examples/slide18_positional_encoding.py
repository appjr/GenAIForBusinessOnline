"""
Week 4: Slide 18 - Positional Encoding

Description: 
    Adds position information to token embeddings.
    Essential for transformers to understand sequence order.

Dependencies:
    - torch
    - numpy
    - matplotlib

Usage:
    python slide18_positional_encoding.py
"""

print("Positional Encoding")
print("Injects position information into embeddings")
print("\nSee week04-slides-batch2.md (Slide 18) for full implementation")
