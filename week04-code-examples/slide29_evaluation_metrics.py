"""
Week 4: Slide 29 - Evaluation Metrics

Description: 
    Metrics for evaluating language models.
    Includes perplexity, BLEU score, and benchmarks.

Dependencies:
    - torch
    - numpy
    - nltk (for BLEU)

Usage:
    python slide29_evaluation_metrics.py
"""

print("Evaluation Metrics")
print("Perplexity, BLEU, and automated benchmarks")
print("\nSee week04-slides-batch3.md (Slide 29) for full implementation")
