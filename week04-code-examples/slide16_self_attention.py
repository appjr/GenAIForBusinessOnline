"""
Week 4: Slide 16 - Self-Attention

Description: 
    Self-attention implementation where each token attends to all others.
    Foundation of transformer architecture.

Dependencies:
    - torch
    - numpy

Usage:
    python slide16_self_attention.py
"""

print("Self-Attention Implementation")
print("Each token attends to all other tokens")
print("\nSee week04-slides-batch2.md (Slide 16) for full implementation")
