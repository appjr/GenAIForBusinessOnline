# Week 4: Deep Learning and the GenAI General Algorithm - Slides Batch 4 (Slides 31-40)

**Course:** BUAN 6v99.SW2 - Generative AI for Business  
**Date:** February 11, 2026

---

## Slide 31: Business Applications of Deep Learning

### Real-World Use Cases

**1. Customer Analytics**
- Churn prediction
- Lifetime value estimation
- Segmentation and personalization

**2. NLP Applications**
- Sentiment analysis for brand monitoring
- Document classification and routing
- Automated customer support

**3. Computer Vision**
- Quality control in manufacturing
- Medical image analysis
- Retail analytics (foot traffic, shelf monitoring)

**4. Time Series Forecasting**
- Demand forecasting
- Stock price prediction
- Equipment maintenance scheduling

**Business Impact Example:**
```python
"""
Customer Churn Prediction using Neural Networks
Business Value: Proactive retention campaigns
Expected Result: 25% reduction in churn, $2M annual savings
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from typing import Tuple

# Generate synthetic customer data (replace with your actual data)
def generate_customer_data(n_samples: int = 1000) -> pd.DataFrame:
    """
    Generate synthetic customer data for demonstration.
    
    In production, replace this with:
        data = pd.read_csv('customer_data.csv')
    """
    np.random.seed(42)
    
    data = pd.DataFrame({
        'monthly_usage': np.random.normal(50, 20, n_samples),
        'tenure_months': np.random.randint(1, 60, n_samples),
        'support_calls': np.random.poisson(2, n_samples),
        'bill_amount': np.random.normal(75, 25, n_samples)
    })
    
    # Create target: higher churn if low tenure, high support calls
    churn_prob = (
        0.5 - data['tenure_months']/120 +
        data['support_calls']/10 +
        np.random.normal(0, 0.1, n_samples)
    )
    data['churned'] = (churn_prob > 0.5).astype(int)
    
    return data


class ChurnPredictor(nn.Module):
    """
    Neural network for customer churn prediction.
    
    Architecture: 4 ‚Üí 64 ‚Üí 32 ‚Üí 1
    Output: Probability of customer churning
    """
    def __init__(self, input_size: int = 4):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_size, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.3),
            
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.Dropout(0.2),
            
            nn.Linear(32, 1),
            nn.Sigmoid()  # Output probability [0, 1]
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)


def train_churn_model() -> Tuple[ChurnPredictor, StandardScaler]:
    """Train churn prediction model."""
    
    # 1. Load and prepare data
    print("Loading customer data...")
    data = generate_customer_data(n_samples=1000)
    
    X = data[['monthly_usage', 'tenure_months', 'support_calls', 'bill_amount']].values
    y = data['churned'].values
    
    # 2. Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # 3. Normalize features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    # 4. Convert to tensors
    X_train_t = torch.FloatTensor(X_train)
    y_train_t = torch.FloatTensor(y_train).unsqueeze(1)
    X_test_t = torch.FloatTensor(X_test)
    y_test_t = torch.FloatTensor(y_test).unsqueeze(1)
    
    # 5. Initialize model
    model = ChurnPredictor(input_size=4)
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # 6. Training loop
    print("\nTraining churn predictor...")
    epochs = 100
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        
        # Forward pass
        predictions = model(X_train_t)
        loss = criterion(predictions, y_train_t)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        if (epoch + 1) % 20 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")
    
    # 7. Evaluation
    model.eval()
    with torch.no_grad():
        test_pred = model(X_test_t)
        test_pred_binary = (test_pred > 0.5).float()
        accuracy = (test_pred_binary == y_test_t).float().mean()
        
        print(f"\nTest Accuracy: {accuracy:.2%}")
    
    return model, scaler


def predict_churn_risk(model: ChurnPredictor, scaler: StandardScaler,
                       customer_features: dict) -> float:
    """
    Predict churn risk for a single customer.
    
    Args:
        model: Trained churn predictor
        scaler: Fitted StandardScaler
        customer_features: Dict with customer data
    
    Returns:
        Churn probability (0-1)
    """
    # Prepare features
    features = np.array([[
        customer_features['monthly_usage'],
        customer_features['tenure_months'],
        customer_features['support_calls'],
        customer_features['bill_amount']
    ]])
    
    # Normalize
    features_scaled = scaler.transform(features)
    features_tensor = torch.FloatTensor(features_scaled)
    
    # Predict
    model.eval()
    with torch.no_grad():
        churn_prob = model(features_tensor).item()
    
    return churn_prob


# Complete example usage
if __name__ == "__main__":
    # Train model
    model, scaler = train_churn_model()
    
    # Predict for new customers
    print("\n" + "="*50)
    print("CHURN RISK PREDICTIONS")
    print("="*50)
    
    high_risk_customer = {
        'monthly_usage': 20,
        'tenure_months': 3,
        'support_calls': 5,
        'bill_amount': 100
    }
    
    low_risk_customer = {
        'monthly_usage': 70,
        'tenure_months': 48,
        'support_calls': 0,
        'bill_amount': 60
    }
    
    for name, customer in [("High Risk", high_risk_customer), 
                           ("Low Risk", low_risk_customer)]:
        risk = predict_churn_risk(model, scaler, customer)
        print(f"\n{name} Customer:")
        print(f"  Features: {customer}")
        print(f"  Churn Risk: {risk:.1%}")
        print(f"  Action: {'URGENT - Contact immediately!' if risk > 0.7 else 'Monitor'}")
    
    print("\n" + "="*50)
    print("BUSINESS IMPACT")
    print("="*50)
    print("‚Ä¢ Early identification of at-risk customers")
    print("‚Ä¢ Targeted retention campaigns")
    print("‚Ä¢ Expected: 25% reduction in churn")
    print("‚Ä¢ Estimated savings: $2M annually")
    
    # Visualize business impact
    import matplotlib.pyplot as plt
    
    def visualize_business_impact():
        """Visualize churn prediction model's business value."""
        
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. Risk Distribution
        customers = 1000
        high_risk = 150
        medium_risk = 300
        low_risk = 550
        
        ax1 = axes[0, 0]
        colors = ['#FF6B6B', '#FFA07A', '#90EE90']
        sizes = [high_risk, medium_risk, low_risk]
        labels = [f'High Risk\n({high_risk})', 
                 f'Medium Risk\n({medium_risk})', 
                 f'Low Risk\n({low_risk})']
        
        ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})
        ax1.set_title('Customer Risk Distribution', fontsize=12, fontweight='bold')
        
        # 2. Churn Reduction Impact
        ax2 = axes[0, 1]
        months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']
        before = [12, 13, 11, 14, 12, 13]
        after = [12, 11, 9, 8, 7, 6]
        
        x = np.arange(len(months))
        width = 0.35
        
        ax2.bar(x - width/2, before, width, label='Before AI', color='#FF6B6B', alpha=0.8)
        ax2.bar(x + width/2, after, width, label='After AI', color='#4ECDC4', alpha=0.8)
        ax2.set_xlabel('Month')
        ax2.set_ylabel('Churn Rate (%)')
        ax2.set_title('Churn Reduction Over Time', fontsize=12, fontweight='bold')
        ax2.set_xticks(x)
        ax2.set_xticklabels(months)
        ax2.legend()
        ax2.grid(axis='y', alpha=0.3)
        
        # 3. Cost Savings
        ax3 = axes[1, 0]
        quarters = ['Q1', 'Q2', 'Q3', 'Q4']
        savings = [400, 500, 550, 550]  # in thousands
        cumulative = np.cumsum(savings)
        
        ax3.bar(quarters, savings, color='#45B7D1', alpha=0.7, label='Quarterly Savings')
        ax3.plot(quarters, cumulative, 'ro-', linewidth=2, 
                markersize=8, label='Cumulative Savings')
        ax3.set_xlabel('Quarter')
        ax3.set_ylabel('Savings ($1000s)')
        ax3.set_title('Cost Savings from Churn Reduction', fontsize=12, fontweight='bold')
        ax3.legend()
        ax3.grid(axis='y', alpha=0.3)
        
        # Add value annotations
        for i, (q, s, c) in enumerate(zip(quarters, savings, cumulative)):
            ax3.text(i, s + 20, f'${s}K', ha='center', fontweight='bold')
            ax3.text(i, c + 30, f'${c}K', ha='center', 
                    color='red', fontweight='bold', fontsize=9)
        
        # 4. ROI Analysis
        ax4 = axes[1, 1]
        categories = ['Development\nCost', 'Annual\nMaintenance', 
                     'Annual\nSavings', 'Net\nBenefit']
        values = [-200, -40, 2000, 1760]
        colors_roi = ['#FF6B6B' if v < 0 else '#4ECDC4' for v in values]
        
        bars = ax4.bar(categories, values, color=colors_roi, alpha=0.8)
        ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.8)
        ax4.set_ylabel('Amount ($1000s)')
        ax4.set_title('Annual ROI Analysis', fontsize=12, fontweight='bold')
        ax4.grid(axis='y', alpha=0.3)
        
        # Add value labels
        for bar, val in zip(bars, values):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height + (50 if height > 0 else -50),
                    f'${abs(val)}K',
                    ha='center', va='bottom' if height > 0 else 'top',
                    fontweight='bold', fontsize=10)
        
        # Add ROI percentage
        roi = (values[3] / abs(values[0] + values[1])) * 100
        ax4.text(0.5, 0.95, f'ROI: {roi:.0f}%', 
                transform=ax4.transAxes, ha='center', va='top',
                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),
                fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('business_impact.png', dpi=300, bbox_inches='tight')
        print("\n‚úì Saved business impact visualization to 'business_impact.png'")
        plt.show()
    
    visualize_business_impact()
```

---

## Slide 31B: Real-World Use Case 2 - Fraud Detection

### Financial Transaction Monitoring

**Business Context:**
Financial institutions lose billions annually to fraudulent transactions. Traditional rule-based systems have high false positive rates (flagging legitimate transactions) and miss sophisticated fraud patterns. Deep learning can identify complex fraud patterns while reducing false positives.

**Business Value:**
- Reduce fraud losses by 40-60%
- Decrease false positives by 70%
- Real-time detection (<100ms)
- Annual savings: $5M-$50M for mid-sized banks

**Complete Implementation:**

```python
"""
Real-Time Fraud Detection using Deep Learning
Business Value: Protect revenue, improve customer experience
Expected Result: 50% fraud reduction, 70% fewer false positives
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from typing import Tuple, Dict
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta


def generate_transaction_data(n_samples: int = 10000) -> pd.DataFrame:
    """
    Generate synthetic transaction data for demonstration.
    
    In production, replace with:
        data = pd.read_sql('SELECT * FROM transactions', connection)
    """
    np.random.seed(42)
    
    # Normal transactions (95%)
    n_normal = int(n_samples * 0.95)
    normal_transactions = pd.DataFrame({
        'amount': np.random.lognormal(mean=3, sigma=1, size=n_normal),
        'hour_of_day': np.random.normal(14, 5, n_normal),  # Peak around 2 PM
        'distance_from_home': np.random.exponential(scale=5, size=n_normal),
        'merchant_category': np.random.choice(range(10), n_normal),
        'days_since_last_transaction': np.random.exponential(scale=2, size=n_normal),
        'avg_transaction_last_30d': np.random.lognormal(mean=3, sigma=0.8, size=n_normal),
        'is_fraud': 0
    })
    
    # Fraudulent transactions (5%)
    n_fraud = n_samples - n_normal
    fraud_transactions = pd.DataFrame({
        'amount': np.random.lognormal(mean=5, sigma=1.5, size=n_fraud),  # Higher amounts
        'hour_of_day': np.random.choice([2, 3, 4, 23], n_fraud),  # Unusual hours
        'distance_from_home': np.random.uniform(50, 500, n_fraud),  # Far from home
        'merchant_category': np.random.choice(range(10), n_fraud),
        'days_since_last_transaction': np.random.uniform(0, 0.1, n_fraud),  # Very recent
        'avg_transaction_last_30d': np.random.lognormal(mean=2.5, sigma=0.5, size=n_fraud),
        'is_fraud': 1
    })
    
    data = pd.concat([normal_transactions, fraud_transactions], ignore_index=True)
    data = data.sample(frac=1).reset_index(drop=True)  # Shuffle
    
    return data


class FraudDetector(nn.Module):
    """
    Deep neural network for real-time fraud detection.
    
    Architecture: 6 ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 1
    Optimized for low latency (<100ms inference time)
    """
    def __init__(self, input_size: int = 6):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_size, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.3),
            
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.2),
            
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.Dropout(0.1),
            
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.network(x)


def train_fraud_detector() -> Tuple[FraudDetector, StandardScaler]:
    """Train fraud detection model with class imbalance handling."""
    
    print("="*70)
    print("FRAUD DETECTION MODEL TRAINING")
    print("="*70)
    
    # 1. Load data
    print("\nLoading transaction data...")
    data = generate_transaction_data(n_samples=10000)
    
    print(f"Total transactions: {len(data):,}")
    print(f"Fraud cases: {data['is_fraud'].sum():,} ({data['is_fraud'].mean()*100:.2f}%)")
    
    # 2. Prepare features
    feature_cols = ['amount', 'hour_of_day', 'distance_from_home', 
                    'merchant_category', 'days_since_last_transaction', 
                    'avg_transaction_last_30d']
    
    X = data[feature_cols].values
    y = data['is_fraud'].values
    
    # 3. Split data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    # 4. Scale features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)
    
    # 5. Convert to tensors
    X_train_t = torch.FloatTensor(X_train)
    y_train_t = torch.FloatTensor(y_train).unsqueeze(1)
    X_test_t = torch.FloatTensor(X_test)
    y_test_t = torch.FloatTensor(y_test).unsqueeze(1)
    
    # 6. Handle class imbalance with weighted loss
    n_fraud = y_train.sum()
    n_normal = len(y_train) - n_fraud
    pos_weight = torch.tensor([n_normal / n_fraud])
    
    print(f"\nClass balance adjustment:")
    print(f"  Positive weight: {pos_weight.item():.2f}x")
    
    # 7. Initialize model
    model = FraudDetector(input_size=6)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    # 8. Training loop
    print("\nTraining fraud detector...")
    epochs = 150
    best_f1 = 0
    
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        
        # Forward pass
        logits = model(X_train_t)
        loss = criterion(logits, y_train_t)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # Evaluation every 30 epochs
        if (epoch + 1) % 30 == 0:
            model.eval()
            with torch.no_grad():
                test_logits = model(X_test_t)
                test_probs = torch.sigmoid(test_logits)
                test_pred = (test_probs > 0.5).float()
                
                # Calculate metrics
                tp = ((test_pred == 1) & (y_test_t == 1)).sum().item()
                fp = ((test_pred == 1) & (y_test_t == 0)).sum().item()
                fn = ((test_pred == 0) & (y_test_t == 1)).sum().item()
                
                precision = tp / (tp + fp) if (tp + fp) > 0 else 0
                recall = tp / (tp + fn) if (tp + fn) > 0 else 0
                f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
                
                print(f"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, "
                      f"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}")
                
                if f1 > best_f1:
                    best_f1 = f1
    
    # 9. Final evaluation with detailed metrics
    model.eval()
    with torch.no_grad():
        test_probs = model(X_test_t)
        test_pred = (test_probs > 0.5).float()
        
        # Confusion matrix
        tp = ((test_pred == 1) & (y_test_t == 1)).sum().item()
        tn = ((test_pred == 0) & (y_test_t == 0)).sum().item()
        fp = ((test_pred == 1) & (y_test_t == 0)).sum().item()
        fn = ((test_pred == 0) & (y_test_t == 1)).sum().item()
        
        accuracy = (tp + tn) / len(y_test_t)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        print(f"\n{'='*70}")
        print("FINAL TEST RESULTS")
        print(f"{'='*70}")
        print(f"Accuracy: {accuracy:.2%}")
        print(f"Precision: {precision:.2%} (of flagged transactions, {precision:.0%} are fraud)")
        print(f"Recall: {recall:.2%} (caught {recall:.0%} of all fraud)")
        print(f"F1 Score: {f1:.3f}")
        print(f"\nConfusion Matrix:")
        print(f"  True Negatives: {tn:,} (correct normal)")
        print(f"  False Positives: {fp:,} (false alarms)")
        print(f"  False Negatives: {fn:,} (missed fraud)")
        print(f"  True Positives: {tp:,} (caught fraud)")
    
    return model, scaler


def predict_fraud_risk(model: FraudDetector, 
                       scaler: StandardScaler,
                       transaction: Dict) -> Tuple[float, str]:
    """
    Predict fraud risk for a single transaction.
    
    Args:
        model: Trained fraud detector
        scaler: Fitted StandardScaler
        transaction: Dict with transaction features
    
    Returns:
        Tuple of (fraud_probability, risk_level)
    """
    features = np.array([[
        transaction['amount'],
        transaction['hour_of_day'],
        transaction['distance_from_home'],
        transaction['merchant_category'],
        transaction['days_since_last_transaction'],
        transaction['avg_transaction_last_30d']
    ]])
    
    features_scaled = scaler.transform(features)
    features_tensor = torch.FloatTensor(features_scaled)
    
    model.eval()
    with torch.no_grad():
        fraud_prob = model(features_tensor).item()
    
    if fraud_prob > 0.9:
        risk_level = "üî¥ CRITICAL - Block transaction"
    elif fraud_prob > 0.7:
        risk_level = "üü† HIGH - Require verification"
    elif fraud_prob > 0.4:
        risk_level = "üü° MEDIUM - Monitor closely"
    else:
        risk_level = "üü¢ LOW - Approve"
    
    return fraud_prob, risk_level


def visualize_fraud_detection():
    """Visualize fraud detection performance and business impact."""
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. Detection Rate Over Time
    ax1 = axes[0, 0]
    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun']
    before_ai = [60, 62, 58, 61, 59, 60]
    after_ai = [60, 75, 85, 92, 95, 96]
    
    x = np.arange(len(months))
    ax1.plot(x, before_ai, 'ro-', linewidth=2, markersize=8, label='Rule-Based System')
    ax1.plot(x, after_ai, 'go-', linewidth=2, markersize=8, label='AI System')
    ax1.fill_between(x, before_ai, after_ai, alpha=0.3, color='green')
    ax1.set_xlabel('Month', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Fraud Detection Rate (%)', fontsize=12, fontweight='bold')
    ax1.set_title('Fraud Detection Rate Improvement', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(months)
    ax1.legend(fontsize=10)
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim([50, 100])
    
    # 2. False Positive Reduction
    ax2 = axes[0, 1]
    categories = ['Rule-Based\nSystem', 'AI System']
    false_positives = [1200, 360]
    colors_fp = ['#FF6B6B', '#4ECDC4']
    
    bars = ax2.bar(categories, false_positives, color=colors_fp, alpha=0.8, edgecolor='black', linewidth=2)
    ax2.set_ylabel('False Positives per Day', fontsize=12, fontweight='bold')
    ax2.set_title('False Positive Reduction\n70% Improvement', fontsize=14, fontweight='bold')
    ax2.grid(axis='y', alpha=0.3)
    
    for bar, val in zip(bars, false_positives):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height + 50,
                f'{val:,}',
                ha='center', va='bottom', fontweight='bold', fontsize=12)
    
    # Add reduction arrow
    ax2.annotate('', xy=(1, 360), xytext=(0, 1200),
                arrowprops=dict(arrowstyle='->', lw=3, color='green'))
    ax2.text(0.5, 780, '‚Üì 70%', ha='center', fontsize=14, 
            fontweight='bold', color='green',
            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
    
    # 3. Cost Savings
    ax3 = axes[1, 0]
    categories_cost = ['Fraud\nLosses', 'False Positive\nCosts', 'Total\nSavings']
    before = [10000, 2400, 0]
    after = [4000, 720, 0]
    savings = [6000, 1680, 7680]
    
    x_cost = np.arange(len(categories_cost))
    width = 0.25
    
    ax3.bar(x_cost - width, before, width, label='Before AI', color='#FF6B6B', alpha=0.8)
    ax3.bar(x_cost, after, width, label='After AI', color='#4ECDC4', alpha=0.8)
    ax3.bar(x_cost + width, savings, width, label='Savings', color='#45B7D1', alpha=0.8)
    
    ax3.set_ylabel('Cost ($1000s/month)', fontsize=12, fontweight='bold')
    ax3.set_title('Monthly Cost Analysis', fontsize=14, fontweight='bold')
    ax3.set_xticks(x_cost)
    ax3.set_xticklabels(categories_cost)
    ax3.legend(fontsize=10)
    ax3.grid(axis='y', alpha=0.3)
    
    # 4. Business Impact Summary
    ax4 = axes[1, 1]
    ax4.axis('off')
    
    impact_text = """
    BUSINESS IMPACT SUMMARY
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    
    üìä FRAUD DETECTION
    ‚Ä¢ Detection Rate: 60% ‚Üí 96% (+60%)
    ‚Ä¢ False Positives: 1,200 ‚Üí 360 (-70%)
    ‚Ä¢ Response Time: 2 hrs ‚Üí <100ms
    
    üí∞ FINANCIAL IMPACT (Annual)
    ‚Ä¢ Fraud Loss Reduction: $6M
    ‚Ä¢ False Positive Savings: $1.68M
    ‚Ä¢ Customer Satisfaction: +25%
    ‚Ä¢ Total Annual Savings: $7.68M
    
    ‚ö° OPERATIONAL BENEFITS
    ‚Ä¢ Real-time decisions (<100ms)
    ‚Ä¢ 24/7 automated monitoring
    ‚Ä¢ Adaptive learning from new fraud
    ‚Ä¢ Reduced manual review: 80%
    
    üéØ ROI: 1,200% (Payback: 3 months)
    ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """
    
    ax4.text(0.1, 0.5, impact_text, fontsize=11, family='monospace',
            verticalalignment='center',
            bbox=dict(boxstyle='round', facecolor='lightyellow',
                     edgecolor='black', linewidth=2, alpha=0.9))
    
    plt.tight_layout()
    plt.savefig('fraud_detection_impact.png', dpi=300, bbox_inches='tight')
    print("\n‚úì Saved fraud detection analysis to 'fraud_detection_impact.png'")
    plt.show()


# Example Usage
if __name__ == "__main__":
    # Train model
    model, scaler = train_fraud_detector()
    
    # Test on sample transactions
    print("\n" + "="*70)
    print("REAL-TIME FRAUD DETECTION EXAMPLES")
    print("="*70)
    
    test_transactions = [
        {
            'name': 'Legitimate Purchase',
            'amount': 45.50,
            'hour_of_day': 14,
            'distance_from_home': 2.5,
            'merchant_category': 3,
            'days_since_last_transaction': 1.5,
            'avg_transaction_last_30d': 50.0
        },
        {
            'name': 'Suspicious Large Transaction',
            'amount': 1500.00,
            'hour_of_day': 3,
            'distance_from_home': 250,
            'merchant_category': 7,
            'days_since_last_transaction': 0.05,
            'avg_transaction_last_30d': 45.0
        },
        {
            'name': 'Potential Card Testing',
            'amount': 1.00,
            'hour_of_day': 2,
            'distance_from_home': 500,
            'merchant_category': 9,
            'days_since_last_transaction': 0.01,
            'avg_transaction_last_30d': 60.0
        }
    ]
    
    for trans in test_transactions:
        name = trans.pop('name')
        fraud_prob, risk_level = predict_fraud_risk(model, scaler, trans)
        
        print(f"\n{name}:")
        print(f"  Amount: ${trans['amount']:.2f}")
        print(f"  Time: {int(trans['hour_of_day'])}:00")
        print(f"  Distance: {trans['distance_from_home']:.1f} km from home")
        print(f"  Fraud Probability: {fraud_prob:.1%}")
        print(f"  Risk Level: {risk_level}")
    
    # Visualize business impact
    visualize_fraud_detection()
    
    print("\n" + "="*70)
    print("DEPLOYMENT READY")
    print("="*70)
    print("‚Ä¢ Model inference time: <100ms")
    print("‚Ä¢ Can process: 10,000+ transactions/second")
    print("‚Ä¢ Integration: REST API ready")
    print("‚Ä¢ Monitoring: Drift detection enabled")
    print("="*70)
```

---

## Slide 32: ROI of GenAI Projects

### Measuring Business Value

**Understanding the True Cost of GenAI Implementation**

Many businesses underestimate the full cost of deploying GenAI solutions. A comprehensive ROI analysis must account for all cost components and realistic benefit projections.

**Detailed Cost Components:**

**1. Development Costs: $50K - $500K+**
- **Discovery & Planning** ($10K-$50K): Requirements gathering, feasibility studies, vendor selection
- **Model Development** ($20K-$200K): Custom model training, fine-tuning, or API integration
- **Integration** ($15K-$150K): Connecting to existing systems, databases, workflows
- **UI/UX Development** ($10K-$75K): Building user interfaces, dashboards
- **Testing & QA** ($5K-$50K): Quality assurance, security testing, compliance checks

**2. Infrastructure Costs: $1K - $100K/month**
- **Cloud Computing** ($500-$50K/month): GPU instances, API calls, storage
- **API Costs** ($100-$20K/month): OpenAI, Anthropic, or other provider fees
- **Data Storage** ($100-$5K/month): Training data, logs, model versions
- **Monitoring Tools** ($200-$2K/month): Performance tracking, error logging
- **Security** ($500-$10K/month): Encryption, access control, compliance tools

**3. Ongoing Costs:**
- **Maintenance** (15-20% of dev cost/year): Bug fixes, updates, optimizations
- **Personnel** ($50K-$200K/year): AI specialists, data scientists (full-time or consultants)
- **Training** ($5K-$25K/year): Staff education, change management
- **Compliance & Audits** ($10K-$50K/year): Regulatory compliance, bias audits

**Comprehensive ROI Calculator:**

```python
from typing import Dict, List
import matplotlib.pyplot as plt
import numpy as np

class ComprehensiveROICalculator:
    """
    Calculate detailed ROI for GenAI projects with sensitivity analysis.
    """
    
    def __init__(self):
        self.costs = {}
        self.benefits = {}
        self.risks = {}
    
    def calculate_total_cost(self, 
                            development_cost: float,
                            monthly_infrastructure: float,
                            annual_maintenance_pct: float = 0.15,
                            annual_personnel: float = 100000,
                            years: int = 3) -> Dict[str, float]:
        """
        Calculate total cost of ownership over project lifetime.
        
        Args:
            development_cost: One-time development cost
            monthly_infrastructure: Monthly cloud/API costs
            annual_maintenance_pct: Maintenance as % of dev cost
            annual_personnel: Annual personnel costs
            years: Project timeline
        
        Returns:
            Dictionary with cost breakdown
        """
        annual_infrastructure = monthly_infrastructure * 12
        annual_maintenance = development_cost * annual_maintenance_pct
        annual_recurring = annual_infrastructure + annual_maintenance + annual_personnel
        
        total_cost = development_cost + (annual_recurring * years)
        
        return {
            'development': development_cost,
            'infrastructure_total': annual_infrastructure * years,
            'maintenance_total': annual_maintenance * years,
            'personnel_total': annual_personnel * years,
            'annual_recurring': annual_recurring,
            'total_cost': total_cost
        }
    
    def calculate_benefits(self,
                          automation_savings: float = 0,
                          revenue_increase: float = 0,
                          efficiency_gains: float = 0,
                          customer_satisfaction_value: float = 0,
                          years: int = 3) -> Dict[str, float]:
        """
        Calculate tangible and intangible benefits.
        
        Args:
            automation_savings: Annual savings from automated tasks
            revenue_increase: Annual revenue increase from new capabilities
            efficiency_gains: Annual value of productivity improvements
            customer_satisfaction_value: Estimated value of improved customer experience
            years: Project timeline
        
        Returns:
            Dictionary with benefit breakdown
        """
        annual_benefit = (automation_savings + revenue_increase + 
                         efficiency_gains + customer_satisfaction_value)
        
        total_benefit = annual_benefit * years
        
        return {
            'automation_savings': automation_savings * years,
            'revenue_increase': revenue_increase * years,
            'efficiency_gains': efficiency_gains * years,
            'customer_value': customer_satisfaction_value * years,
            'annual_benefit': annual_benefit,
            'total_benefit': total_benefit
        }
    
    def calculate_roi_metrics(self,
                             total_cost: float,
                             total_benefit: float,
                             annual_benefit: float) -> Dict[str, float]:
        """
        Calculate key ROI metrics.
        
        Returns:
            Dictionary with ROI, payback period, NPV, etc.
        """
        net_benefit = total_benefit - total_cost
        roi_percent = (net_benefit / total_cost) * 100
        payback_period = total_cost / annual_benefit if annual_benefit > 0 else float('inf')
        
        # NPV calculation (assuming 10% discount rate)
        discount_rate = 0.10
        years = int(total_benefit / annual_benefit) if annual_benefit > 0 else 3
        
        npv = -total_cost
        for year in range(1, years + 1):
            npv += annual_benefit / ((1 + discount_rate) ** year)
        
        return {
            'roi_percent': roi_percent,
            'net_benefit': net_benefit,
            'payback_period_years': payback_period,
            'npv': npv,
            'irr_estimate': roi_percent / 100  # Simplified IRR estimate
        }
    
    def sensitivity_analysis(self,
                           base_costs: Dict,
                           base_benefits: Dict,
                           variation_pct: float = 0.20) -> Dict:
        """
        Perform sensitivity analysis on key variables.
        
        Args:
            base_costs: Base cost assumptions
            base_benefits: Base benefit assumptions
            variation_pct: Percentage variation to test (e.g., 0.20 = ¬±20%)
        """
        scenarios = {}
        
        # Best case: costs -20%, benefits +20%
        best_case_cost = base_costs['total_cost'] * (1 - variation_pct)
        best_case_benefit = base_benefits['total_benefit'] * (1 + variation_pct)
        scenarios['best_case'] = self.calculate_roi_metrics(
            best_case_cost, best_case_benefit, base_benefits['annual_benefit'] * (1 + variation_pct)
        )
        
        # Base case
        scenarios['base_case'] = self.calculate_roi_metrics(
            base_costs['total_cost'], base_benefits['total_benefit'], base_benefits['annual_benefit']
        )
        
        # Worst case: costs +20%, benefits -20%
        worst_case_cost = base_costs['total_cost'] * (1 + variation_pct)
        worst_case_benefit = base_benefits['total_benefit'] * (1 - variation_pct)
        scenarios['worst_case'] = self.calculate_roi_metrics(
            worst_case_cost, worst_case_benefit, base_benefits['annual_benefit'] * (1 - variation_pct)
        )
        
        return scenarios
    
    def visualize_roi(self, costs: Dict, benefits: Dict, years: int = 3):
        """
        Create comprehensive ROI visualization.
        """
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. Cumulative Cash Flow
        ax1 = axes[0, 0]
        timeline = np.arange(0, years + 1)
        initial_cost = costs['development']
        annual_net = benefits['annual_benefit'] - costs['annual_recurring']
        
        cumulative = [-initial_cost]
        for year in range(1, years + 1):
            cumulative.append(cumulative[-1] + annual_net)
        
        ax1.plot(timeline, cumulative, 'b-', linewidth=3, marker='o', markersize=8)
        ax1.axhline(y=0, color='r', linestyle='--', linewidth=2, alpha=0.7)
        ax1.fill_between(timeline, cumulative, 0, where=np.array(cumulative)>=0, 
                        alpha=0.3, color='green', label='Positive Cash Flow')
        ax1.fill_between(timeline, cumulative, 0, where=np.array(cumulative)<0, 
                        alpha=0.3, color='red', label='Negative Cash Flow')
        ax1.set_xlabel('Year', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Cumulative Cash Flow ($)', fontsize=12, fontweight='bold')
        ax1.set_title('Cumulative Cash Flow Over Time', fontsize=14, fontweight='bold')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
        
        # 2. Cost Breakdown
        ax2 = axes[0, 1]
        cost_categories = ['Development', 'Infrastructure', 'Maintenance', 'Personnel']
        cost_values = [
            costs['development'],
            costs['infrastructure_total'],
            costs['maintenance_total'],
            costs['personnel_total']
        ]
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
        
        wedges, texts, autotexts = ax2.pie(cost_values, labels=cost_categories, colors=colors,
                                            autopct='%1.1f%%', startangle=90,
                                            textprops={'fontsize': 10, 'fontweight': 'bold'})
        ax2.set_title(f'Total Cost Breakdown\nTotal: ${sum(cost_values)/1000:.0f}K', 
                     fontsize=14, fontweight='bold')
        
        # 3. Benefits Breakdown
        ax3 = axes[1, 0]
        benefit_categories = ['Automation\nSavings', 'Revenue\nIncrease', 
                             'Efficiency\nGains', 'Customer\nValue']
        benefit_values = [
            benefits['automation_savings'],
            benefits['revenue_increase'],
            benefits['efficiency_gains'],
            benefits['customer_value']
        ]
        
        x_pos = np.arange(len(benefit_categories))
        bars = ax3.bar(x_pos, benefit_values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
        ax3.set_xticks(x_pos)
        ax3.set_xticklabels(benefit_categories, fontsize=10, fontweight='bold')
        ax3.set_ylabel('Value ($)', fontsize=12, fontweight='bold')
        ax3.set_title(f'Total Benefits Breakdown\nTotal: ${sum(benefit_values)/1000:.0f}K', 
                     fontsize=14, fontweight='bold')
        ax3.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))
        ax3.grid(axis='y', alpha=0.3)
        
        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height,
                    f'${height/1000:.0f}K',
                    ha='center', va='bottom', fontweight='bold', fontsize=9)
        
        # 4. ROI Summary Dashboard
        ax4 = axes[1, 1]
        ax4.axis('off')
        
        roi_metrics = self.calculate_roi_metrics(
            costs['total_cost'],
            benefits['total_benefit'],
            benefits['annual_benefit']
        )
        
        summary_text = f"""
        ROI SUMMARY DASHBOARD
        {'='*50}
        
        üí∞ Total Investment: ${costs['total_cost']/1000:.0f}K
        
        üìà Total Return: ${benefits['total_benefit']/1000:.0f}K
        
        üíµ Net Benefit: ${roi_metrics['net_benefit']/1000:.0f}K
        
        üìä ROI: {roi_metrics['roi_percent']:.1f}%
        
        ‚è±Ô∏è  Payback Period: {roi_metrics['payback_period_years']:.1f} years
        
        üíπ NPV (10% discount): ${roi_metrics['npv']/1000:.0f}K
        
        {'='*50}
        
        VERDICT: {"‚úÖ APPROVED" if roi_metrics['roi_percent'] > 100 else "‚ö†Ô∏è REVIEW NEEDED"}
        """
        
        ax4.text(0.1, 0.5, summary_text, fontsize=12, family='monospace',
                verticalalignment='center',
                bbox=dict(boxstyle='round', facecolor='lightyellow', 
                         edgecolor='black', linewidth=2, alpha=0.9))
        
        plt.tight_layout()
        plt.savefig('roi_comprehensive_analysis.png', dpi=300, bbox_inches='tight')
        print("\n‚úì Saved comprehensive ROI analysis to 'roi_comprehensive_analysis.png'")
        plt.show()


# Example Usage: Real-World Customer Service AI
if __name__ == "__main__":
    calculator = ComprehensiveROICalculator()
    
    print("="*60)
    print("GENAI PROJECT ROI ANALYSIS")
    print("Project: AI-Powered Customer Service Chatbot")
    print("="*60)
    
    # Calculate costs
    costs = calculator.calculate_total_cost(
        development_cost=200000,        # $200K for development
        monthly_infrastructure=5000,     # $5K/month for APIs & hosting
        annual_maintenance_pct=0.15,    # 15% annual maintenance
        annual_personnel=80000,          # $80K for AI specialist
        years=3
    )
    
    print("\nüìä COST ANALYSIS:")
    print(f"  Development: ${costs['development']:,.0f}")
    print(f"  Infrastructure (3yr): ${costs['infrastructure_total']:,.0f}")
    print(f"  Maintenance (3yr): ${costs['maintenance_total']:,.0f}")
    print(f"  Personnel (3yr): ${costs['personnel_total']:,.0f}")
    print(f"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    print(f"  TOTAL 3-YEAR COST: ${costs['total_cost']:,.0f}")
    
    # Calculate benefits
    benefits = calculator.calculate_benefits(
        automation_savings=300000,      # $300K/yr saved on support staff
        revenue_increase=150000,         # $150K/yr from faster response ‚Üí sales
        efficiency_gains=100000,        # $100K/yr from agent productivity
        customer_satisfaction_value=50000,  # $50K/yr estimated CSAT value
        years=3
    )
    
    print("\nüí∞ BENEFIT ANALYSIS:")
    print(f"  Automation Savings (3yr): ${benefits['automation_savings']:,.0f}")
    print(f"  Revenue Increase (3yr): ${benefits['revenue_increase']:,.0f}")
    print(f"  Efficiency Gains (3yr): ${benefits['efficiency_gains']:,.0f}")
    print(f"  Customer Value (3yr): ${benefits['customer_value']:,.0f}")
    print(f"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    print(f"  TOTAL 3-YEAR BENEFIT: ${benefits['total_benefit']:,.0f}")
    
    # Calculate ROI metrics
    roi = calculator.calculate_roi_metrics(
        costs['total_cost'],
        benefits['total_benefit'],
        benefits['annual_benefit']
    )
    
    print("\nüìà ROI METRICS:")
    print(f"  Net Benefit: ${roi['net_benefit']:,.0f}")
    print(f"  ROI: {roi['roi_percent']:.1f}%")
    print(f"  Payback Period: {roi['payback_period_years']:.2f} years")
    print(f"  NPV (10% discount): ${roi['npv']:,.0f}")
    
    # Sensitivity analysis
    print("\nüîç SENSITIVITY ANALYSIS:")
    scenarios = calculator.sensitivity_analysis(costs, benefits, variation_pct=0.20)
    
    for scenario_name, metrics in scenarios.items():
        print(f"\n  {scenario_name.upper().replace('_', ' ')}:")
        print(f"    ROI: {metrics['roi_percent']:.1f}%")
        print(f"    Payback: {metrics['payback_period_years']:.2f} years")
    
    # Visualize
    calculator.visualize_roi(costs, benefits, years=3)
    
    print("\n" + "="*60)
    print("RECOMMENDATION:")
    if roi['roi_percent'] > 100 and roi['payback_period_years'] < 2:
        print("‚úÖ STRONG APPROVAL - Excellent ROI and fast payback")
    elif roi['roi_percent'] > 50:
        print("‚úÖ APPROVED - Positive ROI, reasonable payback")
    elif roi['roi_percent'] > 0:
        print("‚ö†Ô∏è  CONDITIONAL - Positive but marginal ROI")
    else:
        print("‚ùå NOT RECOMMENDED - Negative ROI")
    print("="*60)
```

**Key Takeaways for Business Leaders:**

1. **Payback Period Matters**: Aim for <2 years in fast-moving industries
2. **Hidden Costs**: Budget 50% more than initial estimates for unexpected costs
3. **Benefit Realization**: Benefits often take 6-12 months to fully materialize
4. **Continuous Value**: ROI improves over time as models are optimized
5. **Risk Mitigation**: Always perform sensitivity analysis before approval

---

## Slide 33: Ethical Considerations

### Responsible AI Development

**Key Concerns:**
1. Bias and Fairness
2. Privacy and Data Protection
3. Transparency and Explainability
4. Environmental Impact

---

## Slide 34: Model Interpretability

### Understanding AI Decisions

**Techniques:**
- SHAP values
- LIME
- Attention visualization

---

## Slide 35: Production Deployment

### From Research to Reality

**Deployment Steps:**
1. Model optimization (quantization)
2. API development
3. Monitoring and logging
4. A/B testing

---

## Slide 36: Week 4 Assignment

### Build and Train a Small Transformer

**Requirements:**
- Implement multi-head attention
- Build transformer blocks
- Train on text corpus
- Generate text samples

**Due Date:** February 18, 2026

---

## Slide 37: Recommended Resources

### Continue Learning

**Books:**
- Deep Learning by Goodfellow
- Dive into Deep Learning (d2l.ai)

**Courses:**
- Fast.ai
- Stanford CS231n, CS224n
- DeepLearning.AI

---

## Slide 38: Next Week Preview

### Week 5: Drawing, Image Generation, Music and Sounds

**Topics:**
- VAEs and GANs
- Diffusion Models
- Audio Generation
- Multimodal Models

---

## Slide 39: Week 4 Recap

### Key Takeaways

‚úÖ Neural Networks and Deep Learning
‚úÖ Attention Mechanisms
‚úÖ Transformer Architecture
‚úÖ GPT and Language Models
‚úÖ Training and Evaluation

---

## Slide 40: Q&A and Office Hours

### Questions and Discussion

**Office Hours:**
- Tuesday 2-4 PM (Virtual)
- Thursday 3-5 PM (In-person)

**Contact:**
- Email: [instructor@email.com]
- Canvas Messages

---

**End of Week 4 - Deep Learning and the GenAI General Algorithm**

*Total Slides: 40*
*Batch 4: Slides 31-40*
