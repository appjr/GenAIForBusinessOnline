{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "785b6c7f",
   "metadata": {},
   "source": [
    "# Week05 Slides Batch3\n",
    "\n",
    "**Interactive Jupyter Notebook Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27aaab",
   "metadata": {},
   "source": [
    "# Week 5: Image Generation, Audio, and Music - Slides Batch 3 (Slides 16-20)\n",
    "\n",
    "**Course:** BUAN 6v99.SW2 - Generative AI for Business  \n",
    "**Continuation from Batch 2**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc1a45",
   "metadata": {},
   "source": [
    "Introduction to Audio Generation\n",
    "\n",
    "### From Images to Audio - New Challenges\n",
    "\n",
    "**Why Audio is Different from Images:**\n",
    "\n",
    "| Aspect | Images | Audio |\n",
    "|--------|--------|-------|\n",
    "| **Dimensions** | 2D (height × width) | 1D (time) but high sampling rate |\n",
    "| **Sampling** | ~1M pixels (512×512×3) | ~44,100 samples/second |\n",
    "| **Perception** | Spatial, instant | Temporal, sequential |\n",
    "| **Generation** | Independent pixels | Must maintain continuity |\n",
    "| **Quality** | Some blur tolerable | Artifacts very noticeable |\n",
    "\n",
    "**Key Challenge:** Audio requires **high temporal resolution**\n",
    "- 1 second = 44,100 samples\n",
    "- Tiny errors create audible glitches\n",
    "- Must maintain long-range coherence\n",
    "\n",
    "**Audio Representations:**\n",
    "\n",
    "**1. Raw Waveform**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3adbb1",
   "metadata": {},
   "source": [
    "```python\n",
    "Amplitude vs Time\n",
    "Sample rate: 44.1kHz (CD quality)\n",
    "Pros: Direct, lossless\n",
    "Cons: Very high dimensional\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183c63b",
   "metadata": {},
   "source": [
    "**2. Spectrogram**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854b148",
   "metadata": {},
   "source": [
    "```python\n",
    "Frequency × Time\n",
    "Created via Short-Time Fourier Transform (STFT)\n",
    "Pros: Lower dimensional, visual\n",
    "Cons: Phase information lost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b52f1",
   "metadata": {},
   "source": [
    "**3. Mel-Spectrogram**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4330641c",
   "metadata": {},
   "source": [
    "```python\n",
    "Mel-scaled frequency × Time\n",
    "Matches human perception\n",
    "Pros: Perceptually relevant\n",
    "Cons: Need vocoder to convert back\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e281c51",
   "metadata": {},
   "source": [
    "**Audio Processing Basics:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "\n",
    "class AudioProcessor:\n",
    "    \"\"\"\n",
    "    Utilities for audio processing and visualization.\n",
    "    \n",
    "    Handles loading, converting, and visualizing audio.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sample_rate=22050):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sample_rate: Target sampling rate (Hz)\n",
    "        \"\"\"\n",
    "        self.sample_rate = sample_rate\n",
    "    \n",
    "    def load_audio(self, filename):\n",
    "        \"\"\"\n",
    "        Load audio file.\n",
    "        \n",
    "        Args:\n",
    "            filename: Path to audio file\n",
    "        \n",
    "        Returns:\n",
    "            waveform: Audio samples\n",
    "            sample_rate: Sampling rate\n",
    "        \"\"\"\n",
    "        waveform, sr = librosa.load(filename, sr=self.sample_rate)\n",
    "        return waveform, sr\n",
    "    \n",
    "    def save_audio(self, waveform, filename):\n",
    "        \"\"\"Save audio to file\"\"\"\n",
    "        # Normalize to 16-bit range\n",
    "        waveform = np.int16(waveform * 32767)\n",
    "        wavfile.write(filename, self.sample_rate, waveform)\n",
    "    \n",
    "    def compute_spectrogram(self, waveform):\n",
    "        \"\"\"\n",
    "        Compute mel-spectrogram.\n",
    "        \n",
    "        Args:\n",
    "            waveform: Audio samples\n",
    "        \n",
    "        Returns:\n",
    "            mel_spec: Mel-spectrogram\n",
    "        \"\"\"\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=waveform,\n",
    "            sr=self.sample_rate,\n",
    "            n_mels=128,\n",
    "            fmax=8000\n",
    "        )\n",
    "        \n",
    "        # Convert to log scale (dB)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        return mel_spec_db\n",
    "    \n",
    "    def visualize_audio(self, waveform, mel_spec=None):\n",
    "        \"\"\"\n",
    "        Visualize waveform and spectrogram.\n",
    "        \n",
    "        Args:\n",
    "            waveform: Audio samples\n",
    "            mel_spec: Optional mel-spectrogram\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Waveform\n",
    "        time = np.arange(len(waveform)) / self.sample_rate\n",
    "        axes[0].plot(time, waveform)\n",
    "        axes[0].set_xlabel('Time (s)')\n",
    "        axes[0].set_ylabel('Amplitude')\n",
    "        axes[0].set_title('Waveform')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Spectrogram\n",
    "        if mel_spec is None:\n",
    "            mel_spec = self.compute_spectrogram(waveform)\n",
    "        \n",
    "        img = librosa.display.specshow(\n",
    "            mel_spec,\n",
    "            sr=self.sample_rate,\n",
    "            x_axis='time',\n",
    "            y_axis='mel',\n",
    "            ax=axes[1],\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        axes[1].set_title('Mel-Spectrogram')\n",
    "        fig.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    processor = AudioProcessor(sample_rate=22050)\n",
    "    \n",
    "    # Load audio\n",
    "    waveform, sr = processor.load_audio('sample.wav')\n",
    "    print(f\"Loaded {len(waveform)} samples at {sr}Hz\")\n",
    "    print(f\"Duration: {len(waveform)/sr:.2f} seconds\")\n",
    "    \n",
    "    # Compute spectrogram\n",
    "    mel_spec = processor.compute_spectrogram(waveform)\n",
    "    print(f\"Spectrogram shape: {mel_spec.shape}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig = processor.visualize_audio(waveform, mel_spec)\n",
    "    plt.savefig('audio_visualization.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c03e7",
   "metadata": {},
   "source": [
    "**Business Relevance:**\n",
    "- Content creation (podcasts, audiobooks)\n",
    "- Voice assistants\n",
    "- Music production\n",
    "- Accessibility (text-to-speech)\n",
    "- Entertainment (game audio, effects)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c309c70",
   "metadata": {},
   "source": [
    "WaveNet - Direct Waveform Generation\n",
    "\n",
    "### Generating Audio Sample-by-Sample\n",
    "\n",
    "**WaveNet (DeepMind, 2016):**  \n",
    "Revolutionary model that generates raw audio waveforms directly.\n",
    "\n",
    "**Key Innovation:** Dilated Causal Convolutions\n",
    "\n",
    "**Traditional Convolution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54842cef",
   "metadata": {},
   "source": [
    "```python\n",
    "Limited receptive field\n",
    "Can only \"see\" nearby samples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a976f",
   "metadata": {},
   "source": [
    "**Dilated Causal Convolution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c112eb38",
   "metadata": {},
   "source": [
    "```python\n",
    "Exponentially growing receptive field\n",
    "Layer 1: sees 2 samples\n",
    "Layer 2: sees 4 samples  \n",
    "Layer 3: sees 8 samples\n",
    "...\n",
    "Layer 10: sees 1024 samples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90265e",
   "metadata": {},
   "source": [
    "**Architecture:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8011cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Input: Previous audio samples\n",
    "    ↓\n",
    "Dilated Conv Layer 1 (dilation=1)\n",
    "    ↓\n",
    "Dilated Conv Layer 2 (dilation=2)\n",
    "    ↓\n",
    "Dilated Conv Layer 3 (dilation=4)\n",
    "    ↓\n",
    "... (stack of dilated conv layers)\n",
    "    ↓\n",
    "Output: Probability distribution over next sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6984d919",
   "metadata": {},
   "source": [
    "**Simplified WaveNet Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class CausalConv1d(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal 1D convolution - only looks at past samples.\n",
    "    \n",
    "    Ensures autoregressive property: output at time t\n",
    "    only depends on inputs at times < t.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1):\n",
    "        super().__init__()\n",
    "        self.padding = (kernel_size - 1) * dilation\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            padding=self.padding,\n",
    "            dilation=dilation\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with causal padding\"\"\"\n",
    "        x = self.conv(x)\n",
    "        # Remove future samples\n",
    "        if self.padding > 0:\n",
    "            x = x[:, :, :-self.padding]\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with gated activation.\n",
    "    \n",
    "    Core building block of WaveNet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, residual_channels, dilation):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_filter = CausalConv1d(\n",
    "            residual_channels,\n",
    "            residual_channels,\n",
    "            kernel_size=2,\n",
    "            dilation=dilation\n",
    "        )\n",
    "        \n",
    "        self.conv_gate = CausalConv1d(\n",
    "            residual_channels,\n",
    "            residual_channels,\n",
    "            kernel_size=2,\n",
    "            dilation=dilation\n",
    "        )\n",
    "        \n",
    "        self.conv_residual = nn.Conv1d(\n",
    "            residual_channels,\n",
    "            residual_channels,\n",
    "            kernel_size=1\n",
    "        )\n",
    "        \n",
    "        self.conv_skip = nn.Conv1d(\n",
    "            residual_channels,\n",
    "            residual_channels,\n",
    "            kernel_size=1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Gated activation: tanh(filter) ⊙ sigmoid(gate)\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "        \n",
    "        Returns:\n",
    "            residual: For next layer\n",
    "            skip: For skip connection\n",
    "        \"\"\"\n",
    "        # Gated activation\n",
    "        f = self.conv_filter(x)\n",
    "        g = self.conv_gate(x)\n",
    "        z = torch.tanh(f) * torch.sigmoid(g)\n",
    "        \n",
    "        # Residual and skip connections\n",
    "        residual = self.conv_residual(z) + x\n",
    "        skip = self.conv_skip(z)\n",
    "        \n",
    "        return residual, skip\n",
    "\n",
    "\n",
    "class SimpleWaveNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified WaveNet for audio generation.\n",
    "    \n",
    "    Generates audio sample-by-sample autoregressively.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 layers=10,\n",
    "                 blocks=2,\n",
    "                 residual_channels=32,\n",
    "                 quantization_levels=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layers: Number of layers per block\n",
    "            blocks: Number of blocks\n",
    "            residual_channels: Hidden dimension\n",
    "            quantization_levels: Audio quantization (typically 256 for 8-bit)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.quantization_levels = quantization_levels\n",
    "        \n",
    "        # Input embedding\n",
    "        self.input_conv = CausalConv1d(\n",
    "            quantization_levels,\n",
    "            residual_channels,\n",
    "            kernel_size=2\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        for b in range(blocks):\n",
    "            for i in range(layers):\n",
    "                dilation = 2 ** i\n",
    "                self.residual_blocks.append(\n",
    "                    ResidualBlock(residual_channels, dilation)\n",
    "                )\n",
    "        \n",
    "        # Output layers\n",
    "        self.output_conv1 = nn.Conv1d(residual_channels, residual_channels, 1)\n",
    "        self.output_conv2 = nn.Conv1d(residual_channels, quantization_levels, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: One-hot encoded audio (batch, quantization_levels, time)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Predicted distribution over next sample\n",
    "        \"\"\"\n",
    "        # Input embedding\n",
    "        x = self.input_conv(x)\n",
    "        \n",
    "        # Residual blocks with skip connections\n",
    "        skip_connections = []\n",
    "        for block in self.residual_blocks:\n",
    "            x, skip = block(x)\n",
    "            skip_connections.append(skip)\n",
    "        \n",
    "        # Sum skip connections\n",
    "        x = sum(skip_connections)\n",
    "        \n",
    "        # Output\n",
    "        x = F.relu(x)\n",
    "        x = self.output_conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.output_conv2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, length, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate audio sample-by-sample.\n",
    "        \n",
    "        Args:\n",
    "            length: Number of samples to generate\n",
    "            temperature: Sampling temperature (higher = more random)\n",
    "        \n",
    "        Returns:\n",
    "            generated: Generated audio samples\n",
    "        \"\"\"\n",
    "        # Start with silence\n",
    "        generated = torch.zeros(1, self.quantization_levels, 1)\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # Predict next sample\n",
    "            logits = self.forward(generated)[:, :, -1]\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits / temperature, dim=1)\n",
    "            next_sample = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # One-hot encode\n",
    "            next_sample_onehot = F.one_hot(\n",
    "                next_sample,\n",
    "                num_classes=self.quantization_levels\n",
    "            ).float().transpose(1, 2)\n",
    "            \n",
    "            # Append\n",
    "            generated = torch.cat([generated, next_sample_onehot], dim=2)\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "def mu_law_encode(audio, quantization_levels=256):\n",
    "    \"\"\"\n",
    "    μ-law companding for better audio quantization.\n",
    "    \n",
    "    Compresses dynamic range of audio signal.\n",
    "    \"\"\"\n",
    "    mu = quantization_levels - 1\n",
    "    safe_audio = np.minimum(np.maximum(audio, -1.0), 1.0)\n",
    "    magnitude = np.abs(safe_audio)\n",
    "    signal = np.sign(safe_audio) * np.log1p(mu * magnitude) / np.log1p(mu)\n",
    "    return ((signal + 1) / 2 * mu + 0.5).astype(np.int64)\n",
    "\n",
    "\n",
    "def mu_law_decode(quantized, quantization_levels=256):\n",
    "    \"\"\"Decode μ-law encoded audio\"\"\"\n",
    "    mu = quantization_levels - 1\n",
    "    signal = 2 * (quantized / mu) - 1\n",
    "    magnitude = (1 / mu) * ((1 + mu)**np.abs(signal) - 1)\n",
    "    return np.sign(signal) * magnitude\n",
    "\n",
    "\n",
    "# Training example\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"WAVENET FOR AUDIO GENERATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create model\n",
    "    model = SimpleWaveNet(\n",
    "        layers=10,\n",
    "        blocks=2,\n",
    "        residual_channels=32,\n",
    "        quantization_levels=256\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Receptive field: {2**10} samples\")\n",
    "    \n",
    "    # Generate audio\n",
    "    print(\"\\nGenerating audio...\")\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(length=1000, temperature=1.0)\n",
    "    \n",
    "    print(f\"Generated {generated.shape[2]} samples\")\n",
    "    print(\"\\n✓ WaveNet can generate realistic audio!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa124ca5",
   "metadata": {},
   "source": [
    "**Key Features:**\n",
    "\n",
    "1. **Autoregressive:** Generates one sample at a time\n",
    "2. **Causal:** Only uses past samples (no cheating!)\n",
    "3. **Large receptive field:** Can capture long-term dependencies\n",
    "4. **High quality:** Near human-level speech synthesis\n",
    "\n",
    "**Limitations:**\n",
    "- **Very slow:** Must generate sequentially (can't parallelize)\n",
    "- **44,100 samples/second** = slow generation\n",
    "- **Solution:** Parallel WaveNet (teacher-student distillation)\n",
    "\n",
    "**Applications:**\n",
    "- Google Assistant voice\n",
    "- Text-to-speech systems\n",
    "- Music generation\n",
    "- Sound effects\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace87389",
   "metadata": {},
   "source": [
    "Text-to-Speech (TTS)\n",
    "\n",
    "### From Text to Natural Speech\n",
    "\n",
    "**TTS Pipeline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0a2821",
   "metadata": {},
   "source": [
    "```python\n",
    "Text → Text Processing → Acoustic Model → Vocoder → Audio\n",
    "  ↓           ↓               ↓              ↓\n",
    "\"Hello\"   Phonemes      Mel-spectrogram   Waveform\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5ea35d",
   "metadata": {},
   "source": [
    "**Modern TTS: Tacotron 2 + WaveNet**\n",
    "\n",
    "**1. Tacotron 2 (Text → Mel-Spectrogram)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a26805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tacotron2(nn.Module):\n",
    "    \"\"\"\n",
    "    Text-to-Mel-Spectrogram model.\n",
    "    \n",
    "    Architecture:\n",
    "        Text → Encoder → Attention → Decoder → Mel-Spectrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Character embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
    "        \n",
    "        # Encoder: Text → Hidden states\n",
    "        self.encoder = nn.LSTM(\n",
    "            hidden_dim,\n",
    "            hidden_dim // 2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            hidden_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Decoder: Generate mel-spectrogram\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            hidden_dim + 80,  # hidden + mel\n",
    "            hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Mel predictor\n",
    "        self.mel_linear = nn.Linear(hidden_dim, 80)  # 80 mel bands\n",
    "        \n",
    "        # Stop token predictor\n",
    "        self.stop_linear = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Generate mel-spectrogram from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Character indices\n",
    "        \n",
    "        Returns:\n",
    "            mel: Mel-spectrogram\n",
    "            stop_tokens: When to stop generation\n",
    "        \"\"\"\n",
    "        # Encode text\n",
    "        embedded = self.embedding(text)\n",
    "        encoded, _ = self.encoder(embedded)\n",
    "        \n",
    "        # Decode with attention\n",
    "        mel_outputs = []\n",
    "        stop_tokens = []\n",
    "        \n",
    "        # Start with zeros\n",
    "        decoder_input = torch.zeros(text.size(0), 1, 80).to(text.device)\n",
    "        \n",
    "        for _ in range(1000):  # Max length\n",
    "            # Attend to encoder outputs\n",
    "            context, _ = self.attention(\n",
    "                decoder_input,\n",
    "                encoded,\n",
    "                encoded\n",
    "            )\n",
    "            \n",
    "            # Decode\n",
    "            decoder_output, _ = self.decoder_lstm(\n",
    "                torch.cat([context, decoder_input], dim=2)\n",
    "            )\n",
    "            \n",
    "            # Predict mel frame\n",
    "            mel_frame = self.mel_linear(decoder_output)\n",
    "            mel_outputs.append(mel_frame)\n",
    "            \n",
    "            # Predict stop token\n",
    "            stop_prob = torch.sigmoid(self.stop_linear(decoder_output))\n",
    "            stop_tokens.append(stop_prob)\n",
    "            \n",
    "            # Use predicted mel as next input\n",
    "            decoder_input = mel_frame\n",
    "            \n",
    "            # Stop if stop token predicted\n",
    "            if stop_prob > 0.5:\n",
    "                break\n",
    "        \n",
    "        mel = torch.cat(mel_outputs, dim=1)\n",
    "        stops = torch.cat(stop_tokens, dim=1)\n",
    "        \n",
    "        return mel, stops\n",
    "\n",
    "\n",
    "# Complete TTS pipeline\n",
    "class TTSSystem:\n",
    "    \"\"\"\n",
    "    Complete Text-to-Speech system.\n",
    "    \n",
    "    Combines Tacotron 2 (text→mel) with WaveNet (mel→audio).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tacotron = Tacotron2(vocab_size=128)\n",
    "        self.wavenet = SimpleWaveNet()\n",
    "    \n",
    "    def synthesize(self, text):\n",
    "        \"\"\"\n",
    "        Convert text to audio.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text string\n",
    "        \n",
    "        Returns:\n",
    "            audio: Generated waveform\n",
    "        \"\"\"\n",
    "        # Text to character indices\n",
    "        char_indices = torch.tensor([ord(c) for c in text]).unsqueeze(0)\n",
    "        \n",
    "        # Generate mel-spectrogram\n",
    "        with torch.no_grad():\n",
    "            mel, _ = self.tacotron(char_indices)\n",
    "        \n",
    "        # Generate waveform\n",
    "        with torch.no_grad():\n",
    "            audio = self.wavenet.generate_from_mel(mel)\n",
    "        \n",
    "        return audio\n",
    "\n",
    "\n",
    "# Usage\n",
    "tts = TTSSystem()\n",
    "audio = tts.synthesize(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45caf1fd",
   "metadata": {},
   "source": [
    "**2. Modern Alternative: FastSpeech 2**\n",
    "- **Non-autoregressive** (much faster!)\n",
    "- Predicts duration explicitly\n",
    "- Generates entire mel-spec in parallel\n",
    "- 50x faster than Tacotron 2\n",
    "\n",
    "**Real-World TTS APIs:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b93d0e",
   "metadata": {},
   "source": [
    "**Business Applications:**\n",
    "\n",
    "**Content Creation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48229a",
   "metadata": {},
   "source": [
    "**Accessibility:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8725e5",
   "metadata": {},
   "source": [
    "**Virtual Assistants:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ecaf6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa55559",
   "metadata": {},
   "source": [
    "Voice Cloning\n",
    "\n",
    "### Creating Custom Voices\n",
    "\n",
    "**Voice Cloning:** Generate speech in someone's voice using minimal audio samples.\n",
    "\n",
    "**How It Works:**\n",
    "\n",
    "**1. Speaker Embedding**\n",
    "- Extract unique characteristics of a voice\n",
    "- Encode into a vector (embedding)\n",
    "- Capture pitch, tone, accent, style\n",
    "\n",
    "**2. Multi-Speaker TTS**\n",
    "- Train model on many speakers\n",
    "- Condition on speaker embedding\n",
    "- Can generalize to new voices\n",
    "\n",
    "**Architecture:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce81c46",
   "metadata": {},
   "source": [
    "```python\n",
    "Reference Audio → Speaker Encoder → Speaker Embedding\n",
    "                                           ↓\n",
    "Text Input → Tacotron 2 (conditioned on embedding) → Mel-Spec → WaveNet → Audio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd9ee5",
   "metadata": {},
   "source": [
    "**Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38460e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeakerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Extract speaker embedding from audio.\n",
    "    \n",
    "    Maps audio to a fixed-size vector representing speaker identity.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mel_dim=80, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Process mel-spectrogram\n",
    "        self.lstm = nn.LSTM(\n",
    "            mel_dim,\n",
    "            256,\n",
    "            num_layers=3,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Project to embedding\n",
    "        self.linear = nn.Linear(256, embedding_dim)\n",
    "    \n",
    "    def forward(self, mel):\n",
    "        \"\"\"\n",
    "        Extract speaker embedding.\n",
    "        \n",
    "        Args:\n",
    "            mel: Mel-spectrogram of reference audio\n",
    "        \n",
    "        Returns:\n",
    "            embedding: Speaker embedding vector\n",
    "        \"\"\"\n",
    "        _, (hidden, _) = self.lstm(mel)\n",
    "        embedding = self.linear(hidden[-1])\n",
    "        \n",
    "        # L2 normalize\n",
    "        embedding = F.normalize(embedding, p=2, dim=1)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "\n",
    "class MultiSpeakerTTS(nn.Module):\n",
    "    \"\"\"\n",
    "    TTS system that can clone voices.\n",
    "    \n",
    "    Conditions generation on speaker embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.speaker_encoder = SpeakerEncoder(embedding_dim=embedding_dim)\n",
    "        self.tacotron = Tacotron2(vocab_size)\n",
    "        \n",
    "        # Modify Tacotron to accept speaker embedding\n",
    "        # (inject embedding into decoder)\n",
    "    \n",
    "    def clone_voice(self, text, reference_audio):\n",
    "        \"\"\"\n",
    "        Generate speech in voice of reference audio.\n",
    "        \n",
    "        Args:\n",
    "            text: Text to synthesize\n",
    "            reference_audio: Audio sample of target voice\n",
    "        \n",
    "        Returns:\n",
    "            audio: Generated speech in cloned voice\n",
    "        \"\"\"\n",
    "        # Extract speaker embedding from reference\n",
    "        mel_ref = compute_mel(reference_audio)\n",
    "        speaker_embedding = self.speaker_encoder(mel_ref)\n",
    "        \n",
    "        # Generate with speaker conditioning\n",
    "        mel = self.tacotron(text, speaker_embedding)\n",
    "        audio = self.wavenet(mel)\n",
    "        \n",
    "        return audio\n",
    "\n",
    "\n",
    "# Real-world voice cloning APIs\n",
    "def use_voice_cloning(text, voice_sample_path):\n",
    "    \"\"\"\n",
    "    Use commercial voice cloning service.\n",
    "    \n",
    "    Example: ElevenLabs, Resemble.ai, Descript\n",
    "    \"\"\"\n",
    "    # ElevenLabs voice cloning\n",
    "    voice = elevenlabs.clone(\n",
    "        name=\"Custom Voice\",\n",
    "        description=\"Cloned from sample\",\n",
    "        files=[voice_sample_path]\n",
    "    )\n",
    "    \n",
    "    audio = elevenlabs.generate(\n",
    "        text=text,\n",
    "        voice=voice\n",
    "    )\n",
    "    \n",
    "    return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f9512c",
   "metadata": {},
   "source": [
    "**Use Cases:**\n",
    "\n",
    "**1. Content Localization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ce537",
   "metadata": {},
   "source": [
    "**2. Personalized Experiences**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4d63b2",
   "metadata": {},
   "source": [
    "**3. Accessibility**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d61cc",
   "metadata": {},
   "source": [
    "**Ethical Considerations:**\n",
    "\n",
    "⚠️ **Concerns:**\n",
    "- **Deepfakes:** Impersonation, fraud\n",
    "- **Consent:** Using someone's voice without permission\n",
    "- **Misinformation:** Fake audio of public figures\n",
    "\n",
    "✅ **Safeguards:**\n",
    "- Watermarking generated audio\n",
    "- Detection systems for deepfakes\n",
    "- Legal frameworks for consent\n",
    "- Responsible use policies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0729c84",
   "metadata": {},
   "source": [
    "Audio Classification & Understanding\n",
    "\n",
    "### Beyond Generation - Understanding Audio\n",
    "\n",
    "**Audio Classification Tasks:**\n",
    "\n",
    "**1. Speech Recognition (ASR)**\n",
    "- Convert speech to text\n",
    "- Used in: Voice assistants, transcription\n",
    "\n",
    "**2. Speaker Identification**\n",
    "- Who is speaking?\n",
    "- Used in: Security, organization\n",
    "\n",
    "**3. Emotion Recognition**\n",
    "- Detect speaker's emotional state\n",
    "- Used in: Call centers, mental health\n",
    "\n",
    "**4. Sound Event Detection**\n",
    "- Identify sounds (dog bark, car horn, etc.)\n",
    "- Used in: Smart homes, surveillance\n",
    "\n",
    "**5. Music Genre Classification**\n",
    "- Classify music by genre\n",
    "- Used in: Music platforms, recommendation\n",
    "\n",
    "**Audio Classification Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e7d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    General audio classification model.\n",
    "    \n",
    "    Uses CNN on mel-spectrogram for classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_classes, n_mels=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN for processing spectrogram\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Classification head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, n_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, mel_spec):\n",
    "        \"\"\"\n",
    "        Classify audio from mel-spectrogram.\n",
    "        \n",
    "        Args:\n",
    "            mel_spec: Mel-spectrogram (batch, 1, n_mels, time)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Class predictions\n",
    "        \"\"\"\n",
    "        x = self.conv_layers(mel_spec)\n",
    "        x = self.gap(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Business application: Call center quality monitoring\n",
    "class CallCenterMonitor:\n",
    "    \"\"\"\n",
    "    Monitor call center conversations for quality.\n",
    "    \n",
    "    Detects emotions, keywords, compliance issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.emotion_classifier = AudioClassifier(n_classes=7)  # 7 emotions\n",
    "        self.keyword_detector = KeywordSpotter()\n",
    "        self.sentiment_analyzer = SentimentModel()\n",
    "    \n",
    "    def analyze_call(self, audio_path):\n",
    "        \"\"\"\n",
    "        Comprehensive call analysis.\n",
    "        \n",
    "        Args:\n",
    "            audio_path: Path to call recording\n",
    "        \n",
    "        Returns:\n",
    "            report: Analysis report\n",
    "        \"\"\"\n",
    "        # Load audio\n",
    "        audio, sr = librosa.load(audio_path)\n",
    "        mel = compute_mel(audio)\n",
    "        \n",
    "        # Detect emotions\n",
    "        emotions = self.emotion_classifier(mel)\n",
    "        dominant_emotion = emotions.argmax()\n",
    "        \n",
    "        # Detect keywords (compliance, escalation triggers)\n",
    "        keywords_found = self.keyword_detector(audio)\n",
    "        \n",
    "        # Overall sentiment\n",
    "        sentiment = self.sentiment_analyzer(audio)\n",
    "        \n",
    "        report = {\n",
    "            'duration': len(audio) / sr,\n",
    "            'dominant_emotion': dominant_emotion,\n",
    "            'keywords': keywords_found,\n",
    "            'sentiment': sentiment,\n",
    "            'quality_score': self.compute_quality(emotions, keywords_found, sentiment)\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def compute_quality(self, emotions, keywords, sentiment):\n",
    "        \"\"\"Calculate overall call quality score\"\"\"\n",
    "        # Positive emotions + no escalation keywords + positive sentiment = high quality\n",
    "        quality = (\n",
    "            0.4 * (sentiment > 0.5) +\n",
    "            0.3 * (dominant_emotion in [0, 5, 6]) +  # happy, calm, satisfied\n",
    "            0.3 * (1 - len(keywords) / 10)  # fewer issues = better\n",
    "        )\n",
    "        return quality\n",
    "\n",
    "\n",
    "# ROI Example\n",
    "def call_center_roi():\n",
    "    \"\"\"\n",
    "    Calculate ROI of AI call monitoring.\n",
    "    \"\"\"\n",
    "    traditional = {\n",
    "        'human_qa_analysts': 10,\n",
    "        'salary_per_analyst': 50000,\n",
    "        'calls_reviewed_per_analyst': 100,  # per month\n",
    "        'total_cost': 10 * 50000  # $500k/year\n",
    "    }\n",
    "    \n",
    "    ai_system = {\n",
    "        'setup_cost': 50000,  # One-time\n",
    "        'api_costs': 10000,  # per year\n",
    "        'calls_reviewed': 'unlimited',  # 100% of calls\n",
    "        'total_cost': 60000  # first year\n",
    "    }\n",
    "    \n",
    "    savings = traditional['total_cost'] - ai_system['total_cost']\n",
    "    improvement = \"100% coverage vs 1% coverage\"\n",
    "    \n",
    "    print(f\"First Year Savings: ${savings:,}\")\n",
    "    print(f\"Coverage Improvement: {improvement}\")\n",
    "    print(f\"ROI: {(savings / ai_system['total_cost']) * 100:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'savings': savings,\n",
    "        'roi_percent': (savings / ai_system['total_cost']) * 100\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    roi = call_center_roi()\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CALL CENTER AI MONITORING ROI\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Annual Savings: ${roi['savings']:,}\")\n",
    "    print(f\"ROI: {roi['roi_percent']:.1f}%\")\n",
    "    print(\"Additional Benefits:\")\n",
    "    print(\"  • 100% call coverage (vs 1% manual)\")\n",
    "    print(\"  • Real-time insights\")\n",
    "    print(\"  • Compliance monitoring\")\n",
    "    print(\"  • Agent coaching opportunities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd8a22",
   "metadata": {},
   "source": [
    "**Additional Business Applications:**\n",
    "\n",
    "**Music Recommendation:**\n",
    "- Analyze audio features\n",
    "- Classify genre, mood, tempo\n",
    "- Personalized playlists\n",
    "\n",
    "**Smart Home:**\n",
    "- Detect specific sounds (glass breaking, baby crying)\n",
    "- Trigger appropriate responses\n",
    "- Enhanced security\n",
    "\n",
    "**Healthcare:**\n",
    "- Detect breathing abnormalities\n",
    "- Monitor cough patterns\n",
    "- Early disease detection\n",
    "\n",
    "---\n",
    "\n",
    "**End of Batch 3 (Slides 16-20)**\n",
    "\n",
    "*Continue to Batch 4 for Advanced Music Generation (Slides 21-25)*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
