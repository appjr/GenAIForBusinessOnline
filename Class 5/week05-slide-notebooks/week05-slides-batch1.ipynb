{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13b45c8",
   "metadata": {},
   "source": [
    "# Week05 Slides Batch1\n",
    "\n",
    "**Interactive Jupyter Notebook Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8db0589",
   "metadata": {},
   "source": [
    "# Week 5: Image Generation, Audio, and Music - Slides Batch 1 (Slides 1-10)\n",
    "\n",
    "**Course:** BUAN 6v99.SW2 - Generative AI for Business  \n",
    "**Date:** February 17, 2026  \n",
    "**Duration:** 2.5 hours\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc08f5e",
   "metadata": {},
   "source": [
    "Week 5 Title Slide\n",
    "\n",
    "### Image Generation, Audio, and Music with GenAI\n",
    "\n",
    "**Today's Focus:**\n",
    "- Understanding generative models beyond text\n",
    "- VAEs, GANs, and Diffusion Models\n",
    "- Creating images from text prompts\n",
    "- Audio and music generation\n",
    "- Multimodal AI applications\n",
    "- Business use cases in creative industries\n",
    "\n",
    "**Prerequisites:**\n",
    "- Week 4: Deep Learning and Transformers\n",
    "- Understanding of neural networks\n",
    "- Basic PyTorch/TensorFlow knowledge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df30dcf0",
   "metadata": {},
   "source": [
    "Today's Agenda\n",
    "\n",
    "### Class Overview\n",
    "\n",
    "1. **Image Generation Fundamentals** (30 min)\n",
    "2. **VAEs and GANs** (30 min)\n",
    "3. **Break** (10 min)\n",
    "4. **Diffusion Models** (35 min)\n",
    "5. **Audio and Music Generation** (25 min)\n",
    "6. **Business Applications** (20 min)\n",
    "7. **Hands-on Lab & Q&A** (20 min)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5f0a2",
   "metadata": {},
   "source": [
    "Learning Objectives\n",
    "\n",
    "### By the End of This Class, You Will:\n",
    "\n",
    "‚úÖ **Understand** how generative models create images and audio  \n",
    "‚úÖ **Explain** VAEs, GANs, and Diffusion Models  \n",
    "‚úÖ **Implement** basic image generation systems  \n",
    "‚úÖ **Recognize** audio generation techniques  \n",
    "‚úÖ **Apply** these technologies to business problems  \n",
    "‚úÖ **Evaluate** ROI of creative GenAI applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51226e9a",
   "metadata": {},
   "source": [
    "The Evolution of Image Generation\n",
    "\n",
    "### From Rules to Neural Networks\n",
    "\n",
    "**Historical Timeline:**\n",
    "\n",
    "**1. Rule-Based Graphics (1960s-1990s)**\n",
    "- Computer graphics with manual programming\n",
    "- 3D rendering engines\n",
    "- Procedural generation\n",
    "- Limited creativity, deterministic\n",
    "\n",
    "**2. Style Transfer (2015)**\n",
    "- Neural Style Transfer using CNNs\n",
    "- Combine content of one image with style of another\n",
    "- First neural network \"art\"\n",
    "- Example: Photo in Van Gogh's style\n",
    "\n",
    "**3. GANs (2014-2020)**\n",
    "- Generate realistic faces, objects, scenes\n",
    "- Progressive improvements (StyleGAN)\n",
    "- High-resolution synthesis\n",
    "- Limited control over output\n",
    "\n",
    "**4. Diffusion Models (2020-Present)**\n",
    "- DALL-E, Stable Diffusion, Midjourney\n",
    "- Text-to-image generation\n",
    "- Precise control via prompts\n",
    "- Revolutionary creative tool\n",
    "\n",
    "**Business Impact:**\n",
    "- Design automation\n",
    "- Content creation at scale\n",
    "- Personalization\n",
    "- Cost reduction: $50-200 per image ‚Üí $0.01\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30ec2d",
   "metadata": {},
   "source": [
    "Variational Autoencoders (VAEs)\n",
    "\n",
    "### Learning Compressed Representations\n",
    "\n",
    "**What is a VAE?**\n",
    "\n",
    "A Variational Autoencoder learns to:\n",
    "1. **Encode** images into a compressed latent space\n",
    "2. **Sample** from that latent space\n",
    "3. **Decode** samples back into images\n",
    "\n",
    "**Architecture:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656921c5",
   "metadata": {},
   "source": [
    "```python\n",
    "Image ‚Üí Encoder ‚Üí Latent Space (Œº, œÉ) ‚Üí Decoder ‚Üí Reconstructed Image\n",
    "                      ‚Üì\n",
    "                   Sample z ~ N(Œº, œÉ)\n",
    "                      ‚Üì\n",
    "                   Decoder ‚Üí New Image\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1f48d",
   "metadata": {},
   "source": [
    "**Key Insight:** The latent space is **continuous and smooth**, meaning:\n",
    "- Similar images cluster together\n",
    "- Interpolation between images is meaningful\n",
    "- We can sample to generate new images\n",
    "\n",
    "**Complete VAE Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2403217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Variational Autoencoder for image generation.\n",
    "    \n",
    "    Architecture:\n",
    "        Encoder: Image ‚Üí Latent distribution (Œº, log_var)\n",
    "        Decoder: Latent sample ‚Üí Reconstructed image\n",
    "    \n",
    "    Loss:\n",
    "        Reconstruction loss + KL divergence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=20, input_channels=1, hidden_dim=400):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent_dim: Dimension of latent space\n",
    "            input_channels: 1 for grayscale, 3 for RGB\n",
    "            hidden_dim: Hidden layer dimension\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: Image ‚Üí Hidden ‚Üí Latent parameters\n",
    "        self.fc1 = nn.Linear(input_channels * 28 * 28, hidden_dim)\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder: Latent ‚Üí Hidden ‚Üí Image\n",
    "        self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, input_channels * 28 * 28)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode image to latent distribution parameters.\n",
    "        \n",
    "        Args:\n",
    "            x: Input images (batch, channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            mu: Mean of latent distribution\n",
    "            logvar: Log variance of latent distribution\n",
    "        \"\"\"\n",
    "        # Flatten image\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        \n",
    "        # Encoder forward pass\n",
    "        h = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = Œº + œÉ * Œµ, where Œµ ~ N(0,1)\n",
    "        \n",
    "        This allows backpropagation through sampling.\n",
    "        \n",
    "        Args:\n",
    "            mu: Mean\n",
    "            logvar: Log variance\n",
    "        \n",
    "        Returns:\n",
    "            z: Sampled latent vector\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent vector to image.\n",
    "        \n",
    "        Args:\n",
    "            z: Latent vector\n",
    "        \n",
    "        Returns:\n",
    "            Reconstructed image\n",
    "        \"\"\"\n",
    "        h = F.relu(self.fc3(z))\n",
    "        x_reconstructed = torch.sigmoid(self.fc4(h))\n",
    "        return x_reconstructed.view(-1, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Full forward pass: encode, sample, decode\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_reconstructed = self.decode(z)\n",
    "        return x_reconstructed, mu, logvar\n",
    "\n",
    "\n",
    "def vae_loss(x_reconstructed, x, mu, logvar):\n",
    "    \"\"\"\n",
    "    VAE loss = Reconstruction loss + KL divergence\n",
    "    \n",
    "    Reconstruction loss: How well we reconstruct input\n",
    "    KL divergence: How close latent distribution is to N(0,1)\n",
    "    \n",
    "    Args:\n",
    "        x_reconstructed: Reconstructed images\n",
    "        x: Original images\n",
    "        mu: Mean of latent distribution\n",
    "        logvar: Log variance of latent distribution\n",
    "    \n",
    "    Returns:\n",
    "        Total loss\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (binary cross-entropy)\n",
    "    BCE = F.binary_cross_entropy(\n",
    "        x_reconstructed.view(-1, 28*28), \n",
    "        x.view(-1, 28*28), \n",
    "        reduction='sum'\n",
    "    )\n",
    "    \n",
    "    # KL divergence: KL(N(Œº,œÉ¬≤) || N(0,1))\n",
    "    # = -0.5 * sum(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + KLD\n",
    "\n",
    "\n",
    "def train_vae(model, train_loader, optimizer, epoch, device):\n",
    "    \"\"\"Train VAE for one epoch\"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        x_reconstructed, mu, logvar = model(data)\n",
    "        loss = vae_loss(x_reconstructed, data, mu, logvar)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '\n",
    "                  f'Loss: {loss.item() / len(data):.4f}')\n",
    "    \n",
    "    avg_loss = train_loss / len(train_loader.dataset)\n",
    "    print(f'====> Epoch: {epoch} Average loss: {avg_loss:.4f}')\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# Complete training example\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"VARIATIONAL AUTOENCODER (VAE) TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    \n",
    "    # Load MNIST\n",
    "    print(\"\\nLoading MNIST dataset...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    # Create model\n",
    "    latent_dim = 20\n",
    "    model = VAE(latent_dim=latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    print(f\"\\nModel Configuration:\")\n",
    "    print(f\"  Latent dimension: {latent_dim}\")\n",
    "    print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    n_epochs = 10\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_vae(model, train_loader, optimizer, epoch, device)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Generate new images\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING NEW IMAGES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Sample from standard normal\n",
    "        z = torch.randn(64, latent_dim).to(device)\n",
    "        samples = model.decode(z).cpu()\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            ax.imshow(samples[i].squeeze(), cmap='gray')\n",
    "            ax.axis('off')\n",
    "        \n",
    "        plt.suptitle('Generated Images from VAE', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('vae_generated_images.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"\\n‚úì Saved generated images to 'vae_generated_images.png'\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Visualize latent space\n",
    "    print(\"\\nInterpolating in latent space...\")\n",
    "    with torch.no_grad():\n",
    "        # Two random points\n",
    "        z1 = torch.randn(1, latent_dim).to(device)\n",
    "        z2 = torch.randn(1, latent_dim).to(device)\n",
    "        \n",
    "        # Interpolate\n",
    "        fig, axes = plt.subplots(1, 10, figsize=(20, 2))\n",
    "        for i, alpha in enumerate(np.linspace(0, 1, 10)):\n",
    "            z = (1 - alpha) * z1 + alpha * z2\n",
    "            img = model.decode(z).cpu()\n",
    "            axes[i].imshow(img.squeeze(), cmap='gray')\n",
    "            axes[i].axis('off')\n",
    "            axes[i].set_title(f'{alpha:.1f}')\n",
    "        \n",
    "        plt.suptitle('Latent Space Interpolation', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('vae_interpolation.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Saved interpolation to 'vae_interpolation.png'\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "    print(\"Model can now generate new images by sampling from latent space!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e9060",
   "metadata": {},
   "source": [
    "**Key Advantages:**\n",
    "- ‚úÖ Smooth latent space (good for interpolation)\n",
    "- ‚úÖ Probabilistic framework (quantifies uncertainty)\n",
    "- ‚úÖ Easy to train (stable)\n",
    "\n",
    "**Limitations:**\n",
    "- ‚ùå Blurry outputs (due to reconstruction loss)\n",
    "- ‚ùå Less realistic than GANs\n",
    "- ‚ùå Limited diversity\n",
    "\n",
    "**Business Applications:**\n",
    "- Product design variations\n",
    "- Data augmentation\n",
    "- Anomaly detection\n",
    "- Dimensionality reduction\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22672bcc",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs)\n",
    "\n",
    "### A Game Between Generator and Discriminator\n",
    "\n",
    "**What is a GAN?**\n",
    "\n",
    "Two neural networks compete in a game:\n",
    "\n",
    "**Generator (G):**\n",
    "- Creates fake images from random noise\n",
    "- Goal: Fool the discriminator\n",
    "- \"Counterfeiter making fake money\"\n",
    "\n",
    "**Discriminator (D):**\n",
    "- Distinguishes real from fake images\n",
    "- Goal: Detect generator's fakes\n",
    "- \"Police detecting counterfeit money\"\n",
    "\n",
    "**Training Process:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5462a005",
   "metadata": {},
   "source": [
    "```python\n",
    "1. Generator creates fake images\n",
    "2. Discriminator sees real + fake images\n",
    "3. Discriminator learns to tell them apart\n",
    "4. Generator learns to fool discriminator better\n",
    "5. Repeat until generator produces realistic images\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4741fc",
   "metadata": {},
   "source": [
    "**Mathematical Framework:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe92e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_G max_D V(D,G) = E_x[log D(x)] + E_z[log(1 - D(G(z)))]\n",
    "\n",
    "Where:\n",
    "- D(x): Discriminator's probability that x is real\n",
    "- G(z): Generator's output from noise z\n",
    "- Real images maximize D(x)\n",
    "- Fake images minimize D(G(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23a358",
   "metadata": {},
   "source": [
    "**Complete GAN Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079f6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator network: Noise ‚Üí Image\n",
    "    \n",
    "    Takes random noise and generates realistic images.\n",
    "    Uses transposed convolutions to upsample.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=100, img_channels=1, feature_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            latent_dim: Dimension of input noise vector\n",
    "            img_channels: Number of image channels (1 for grayscale, 3 for RGB)\n",
    "            feature_dim: Base feature map size\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Network: Noise (100) ‚Üí 7x7x256 ‚Üí 14x14x128 ‚Üí 28x28x1\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: (batch, latent_dim, 1, 1)\n",
    "            nn.ConvTranspose2d(latent_dim, feature_dim * 4, 7, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(feature_dim * 4),\n",
    "            nn.ReLU(True),\n",
    "            # State: (batch, feature_dim*4, 7, 7)\n",
    "            \n",
    "            nn.ConvTranspose2d(feature_dim * 4, feature_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_dim * 2),\n",
    "            nn.ReLU(True),\n",
    "            # State: (batch, feature_dim*2, 14, 14)\n",
    "            \n",
    "            nn.ConvTranspose2d(feature_dim * 2, img_channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output: (batch, img_channels, 28, 28), values in [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"Generate image from noise\"\"\"\n",
    "        img = self.model(z)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator network: Image ‚Üí Real/Fake probability\n",
    "    \n",
    "    Classifies images as real or fake.\n",
    "    Uses standard convolutions to downsample.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_channels=1, feature_dim=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_channels: Number of image channels\n",
    "            feature_dim: Base feature map size\n",
    "        \"\"\"\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # Network: 28x28x1 ‚Üí 14x14x64 ‚Üí 7x7x128 ‚Üí 1\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: (batch, img_channels, 28, 28)\n",
    "            nn.Conv2d(img_channels, feature_dim, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: (batch, feature_dim, 14, 14)\n",
    "            \n",
    "            nn.Conv2d(feature_dim, feature_dim * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(feature_dim * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: (batch, feature_dim*2, 7, 7)\n",
    "            \n",
    "            nn.Conv2d(feature_dim * 2, 1, 7, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # Output: (batch, 1, 1, 1) ‚Üí probability\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        \"\"\"Classify image as real or fake\"\"\"\n",
    "        validity = self.model(img)\n",
    "        return validity.view(-1, 1)\n",
    "\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, n_epochs, latent_dim, device):\n",
    "    \"\"\"\n",
    "    Train GAN using alternating optimization.\n",
    "    \n",
    "    Training loop:\n",
    "        1. Train Discriminator on real + fake images\n",
    "        2. Train Generator to fool Discriminator\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loss function: Binary Cross-Entropy\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    # Training history\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GAN TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (real_imgs, _) in enumerate(dataloader):\n",
    "            batch_size = real_imgs.size(0)\n",
    "            real_imgs = real_imgs.to(device)\n",
    "            \n",
    "            # Labels for real and fake\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            optimizer_D.zero_grad()\n",
    "            \n",
    "            # Real images\n",
    "            real_validity = discriminator(real_imgs)\n",
    "            d_real_loss = criterion(real_validity, real_labels)\n",
    "            \n",
    "            # Fake images\n",
    "            z = torch.randn(batch_size, latent_dim, 1, 1).to(device)\n",
    "            fake_imgs = generator(z)\n",
    "            fake_validity = discriminator(fake_imgs.detach())\n",
    "            d_fake_loss = criterion(fake_validity, fake_labels)\n",
    "            \n",
    "            # Total discriminator loss\n",
    "            d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "            \n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_G.zero_grad()\n",
    "            \n",
    "            # Generate images and try to fool discriminator\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = criterion(fake_validity, real_labels)  # Want discriminator to think they're real\n",
    "            \n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Logging\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch}/{n_epochs}] Batch [{i}/{len(dataloader)}] \"\n",
    "                      f\"D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}\")\n",
    "        \n",
    "        d_losses.append(d_loss.item())\n",
    "        g_losses.append(g_loss.item())\n",
    "        \n",
    "        # Generate samples every epoch\n",
    "        if epoch % 5 == 0:\n",
    "            with torch.no_grad():\n",
    "                z = torch.randn(16, latent_dim, 1, 1).to(device)\n",
    "                gen_imgs = generator(z).cpu()\n",
    "                \n",
    "                fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "                for idx, ax in enumerate(axes.flat):\n",
    "                    ax.imshow(gen_imgs[idx].squeeze(), cmap='gray')\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle(f'Generated Images - Epoch {epoch}', fontsize=14, fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'gan_epoch_{epoch}.png', dpi=150)\n",
    "                plt.close()\n",
    "    \n",
    "    return d_losses, g_losses\n",
    "\n",
    "\n",
    "# Complete example\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"GENERATIVE ADVERSARIAL NETWORK (GAN) TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    \n",
    "    # Hyperparameters\n",
    "    latent_dim = 100\n",
    "    batch_size = 128\n",
    "    n_epochs = 50\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\nLoading MNIST dataset...\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Create models\n",
    "    generator = Generator(latent_dim=latent_dim).to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "    \n",
    "    print(f\"\\nGenerator parameters: {sum(p.numel() for p in generator.parameters()):,}\")\n",
    "    print(f\"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}\")\n",
    "    \n",
    "    # Train\n",
    "    d_losses, g_losses = train_gan(\n",
    "        generator, discriminator, dataloader, \n",
    "        n_epochs, latent_dim, device\n",
    "    )\n",
    "    \n",
    "    # Visualize training\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(d_losses, label='Discriminator Loss')\n",
    "    plt.plot(g_losses, label='Generator Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('GAN Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig('gan_training_losses.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"‚úì Generator can now create realistic images from random noise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40144325",
   "metadata": {},
   "source": [
    "**Key Advantages:**\n",
    "- ‚úÖ Sharp, realistic images\n",
    "- ‚úÖ High quality outputs\n",
    "- ‚úÖ Versatile (images, video, audio)\n",
    "\n",
    "**Limitations:**\n",
    "- ‚ùå Training instability (mode collapse)\n",
    "- ‚ùå Difficult to converge\n",
    "- ‚ùå Limited control over outputs\n",
    "\n",
    "**Famous GANs:**\n",
    "- StyleGAN (faces)\n",
    "- BigGAN (ImageNet)\n",
    "- Pix2Pix (image-to-image)\n",
    "- CycleGAN (unpaired translation)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23dd3ea",
   "metadata": {},
   "source": [
    "GAN Applications\n",
    "\n",
    "### Real-World Business Use Cases\n",
    "\n",
    "**1. Face Generation (StyleGAN)**\n",
    "- Create realistic human faces\n",
    "- Adjust age, gender, expression\n",
    "- Use: Avatars, game characters, privacy\n",
    "\n",
    "**2. Image-to-Image Translation**\n",
    "- Pix2Pix: Sketch ‚Üí Photo\n",
    "- CycleGAN: Summer ‚Üí Winter, Horse ‚Üí Zebra\n",
    "- Use: Design automation, video effects\n",
    "\n",
    "**3. Super-Resolution**\n",
    "- Enhance low-resolution images\n",
    "- Restore old photos\n",
    "- Use: Medical imaging, surveillance\n",
    "\n",
    "**4. Deepfakes (Ethical Concerns)**\n",
    "- Face swapping in videos\n",
    "- Voice cloning\n",
    "- Use: Entertainment, BUT major ethical issues\n",
    "\n",
    "**Business ROI Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09c1005",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6987b57",
   "metadata": {},
   "source": [
    "Mode Collapse in GANs\n",
    "\n",
    "### A Common Training Problem\n",
    "\n",
    "**What is Mode Collapse?**\n",
    "\n",
    "Generator learns to produce only a few types of outputs (modes) instead of the full diversity of the data distribution.\n",
    "\n",
    "**Example:**\n",
    "- Training on faces dataset with thousands of unique people\n",
    "- Generator only produces 10-20 different faces\n",
    "- High quality but low diversity\n",
    "\n",
    "**Why It Happens:**\n",
    "1. Generator finds a few outputs that fool discriminator\n",
    "2. Keeps generating those \"winning\" outputs\n",
    "3. Doesn't explore other possibilities\n",
    "4. Gets stuck in local optimum\n",
    "\n",
    "**Visual Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2001cca",
   "metadata": {},
   "source": [
    "```python\n",
    "Desired:  üòÄ üòÉ üòÑ üòÅ üòÜ üòÖ üòÇ ü§£ üòä üòá ... (1000s of variations)\n",
    "Reality:  üòÄ üòÄ üòÄ üòÉ üòÉ üòÉ üòÑ üòÑ üòÑ  (only 3 variations)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c0fae8",
   "metadata": {},
   "source": [
    "**Solutions:**\n",
    "\n",
    "**1. Minibatch Discrimination**\n",
    "- Discriminator compares images within a batch\n",
    "- Penalizes if all images are similar\n",
    "- Encourages diversity\n",
    "\n",
    "**2. Unrolled GANs**\n",
    "- Generator looks ahead several discriminator update steps\n",
    "- Prevents short-term exploitation\n",
    "\n",
    "**3. Wasserstein GAN (WGAN)**\n",
    "- Different loss function (Earth Mover's Distance)\n",
    "- More stable training\n",
    "- Meaningful loss curves\n",
    "\n",
    "**4. Conditional GANs**\n",
    "- Add class labels as input\n",
    "- Force generator to produce specific types\n",
    "- Better control and diversity\n",
    "\n",
    "**Business Impact:**\n",
    "- Mode collapse ‚Üí Limited usefulness\n",
    "- Stable training ‚Üí Reliable deployment\n",
    "- Diversity ‚Üí Better creative applications\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbcf24",
   "metadata": {},
   "source": [
    "Conditional GANs (cGANs)\n",
    "\n",
    "### Adding Control to Generation\n",
    "\n",
    "**Problem with Standard GANs:**\n",
    "- Can't control what gets generated\n",
    "- Random noise ‚Üí random output\n",
    "- No way to specify \"generate a 7\" or \"make it blue\"\n",
    "\n",
    "**Solution: Conditional GANs**\n",
    "\n",
    "Add additional information (labels, attributes) to both generator and discriminator:\n",
    "\n",
    "**Architecture:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c729d462",
   "metadata": {},
   "source": [
    "```python\n",
    "Generator:  (Noise + Label) ‚Üí Image\n",
    "Discriminator: (Image + Label) ‚Üí Real/Fake\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af122228",
   "metadata": {},
   "source": [
    "**Implementation Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1aff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGenerator(nn.Module):\n",
    "    \"\"\"Generator that takes class label as additional input\"\"\"\n",
    "    def __init__(self, latent_dim=100, n_classes=10, img_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding for class labels\n",
    "        self.label_emb = nn.Embedding(n_classes, latent_dim)\n",
    "        \n",
    "        # Generator (now takes latent_dim * 2)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 28 * 28),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, noise, labels):\n",
    "        # Concatenate noise and embedded labels\n",
    "        label_embedding = self.label_emb(labels)\n",
    "        combined_input = torch.cat([noise, label_embedding], dim=1)\n",
    "        \n",
    "        # Generate image\n",
    "        img = self.model(combined_input)\n",
    "        img = img.view(-1, 1, 28, 28)\n",
    "        return img\n",
    "\n",
    "\n",
    "# Usage: Generate specific digit\n",
    "generator = ConditionalGenerator()\n",
    "noise = torch.randn(1, 100)\n",
    "label = torch.tensor([7])  # Generate a \"7\"\n",
    "generated_seven = generator(noise, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead8a38",
   "metadata": {},
   "source": [
    "**Applications:**\n",
    "\n",
    "**1. Text-to-Image (like DALL-E)**\n",
    "- Text prompt as condition\n",
    "- Generate images matching description\n",
    "- Revolutionary for creative industries\n",
    "\n",
    "**2. Image-to-Image Translation**\n",
    "- Pix2Pix: Sketch + \"make realistic\" ‚Üí Photo\n",
    "- Edges + \"add colors\" ‚Üí Colored image\n",
    "\n",
    "**3. Style Transfer**\n",
    "- Content image + Style reference ‚Üí Stylized image\n",
    "- Preserve content, change artistic style\n",
    "\n",
    "**4. Face Editing**\n",
    "- Face + Attributes (add glasses, smile) ‚Üí Modified face\n",
    "- Precise control over generation\n",
    "\n",
    "**Business Use Case:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296f0de4",
   "metadata": {},
   "source": [
    "```python\n",
    "E-commerce Product Images:\n",
    "- Input: Product sketch + \"professional studio lighting\"\n",
    "- Output: High-quality product photo\n",
    "- Saves: $100-500 per product photoshoot\n",
    "- ROI: 95% cost reduction for 1000+ products\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd4563",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd724b4d",
   "metadata": {},
   "source": [
    "Diffusion Models Introduction\n",
    "\n",
    "### The New State-of-the-Art\n",
    "\n",
    "**What Changed Everything: Diffusion Models (2020+)**\n",
    "\n",
    "Diffusion models are now the leading approach for image generation, powering:\n",
    "- DALL-E 2 & 3\n",
    "- Stable Diffusion\n",
    "- Midjourney\n",
    "- Imagen (Google)\n",
    "\n",
    "**The Big Idea:**\n",
    "\n",
    "**Forward Process (Adding Noise):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a655e4",
   "metadata": {},
   "source": [
    "```python\n",
    "Clean Image ‚Üí Add noise ‚Üí Add more noise ‚Üí ... ‚Üí Pure noise\n",
    "    ‚Üì            ‚Üì              ‚Üì                      ‚Üì\n",
    "  Step 0      Step 1        Step 2              Step 1000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec17ddd",
   "metadata": {},
   "source": [
    "**Reverse Process (Denoising):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d709a6",
   "metadata": {},
   "source": [
    "```python\n",
    "Pure noise ‚Üí Denoise ‚Üí Denoise more ‚Üí ... ‚Üí Clean Image\n",
    "    ‚Üì          ‚Üì            ‚Üì                    ‚Üì\n",
    " Step 1000  Step 999    Step 998            Step 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5da54",
   "metadata": {},
   "source": [
    "**Key Insight:** If we learn to reverse the noise-adding process, we can generate images from pure noise!\n",
    "\n",
    "**Why Diffusion Models Won:**\n",
    "\n",
    "‚úÖ **Better Quality:** More realistic than GANs\n",
    "‚úÖ **More Stable:** Easier to train than GANs\n",
    "‚úÖ **Better Control:** Text conditioning works excellently\n",
    "‚úÖ **Scalable:** Works well with massive models\n",
    "‚úÖ **Diverse:** No mode collapse issues\n",
    "\n",
    "**The Numbers:**\n",
    "- DALL-E 2: 3.5B parameters\n",
    "- Stable Diffusion: 860M parameters\n",
    "- Training cost: $50M+ for DALL-E 2\n",
    "- Business impact: $10B+ market by 2028\n",
    "\n",
    "**Simple Analogy:**\n",
    "\n",
    "Think of it like restoring an old, damaged photo:\n",
    "1. Start with extremely noisy image (like static on TV)\n",
    "2. AI gradually removes noise, revealing structure\n",
    "3. Each step makes image slightly clearer\n",
    "4. After 1000 steps: Perfect, new image\n",
    "\n",
    "**Coming up:** Full implementation in Slide 11-13!\n",
    "\n",
    "---\n",
    "\n",
    "**End of Batch 1 (Slides 1-10)**\n",
    "\n",
    "*Continue to Batch 2 for Diffusion Model Implementation and Audio Generation*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
