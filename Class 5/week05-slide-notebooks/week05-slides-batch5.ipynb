{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64ac72b9",
   "metadata": {},
   "source": [
    "# Week05 Slides Batch5\n",
    "\n",
    "**Interactive Jupyter Notebook Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab122bd",
   "metadata": {},
   "source": [
    "# Week 5: Image Generation, Audio, and Music - Slides Batch 5 (Slides 26-30)\n",
    "\n",
    "**Course:** BUAN 6v99.SW2 - Generative AI for Business  \n",
    "**Continuation from Batch 4**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c5b1ad",
   "metadata": {},
   "source": [
    "Multimodal Generation - Combining Vision and Audio\n",
    "\n",
    "### When Images Meet Sound\n",
    "\n",
    "**Multimodal AI:** Systems that work with multiple types of data simultaneously.\n",
    "\n",
    "**Key Applications:**\n",
    "- Video generation (images + audio)\n",
    "- Image-to-audio (generate sounds from images)\n",
    "- Audio-to-image (visualize audio)\n",
    "- Cross-modal retrieval\n",
    "\n",
    "**Video Generation Pipeline:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a794cc1",
   "metadata": {},
   "source": [
    "```python\n",
    "Text Prompt → Video Frames (Diffusion) + Audio (MusicLM) → Synchronized Video\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e254d8d2",
   "metadata": {},
   "source": [
    "**Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba0e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalGenerator:\n",
    "    \"\"\"\n",
    "    Generate synchronized video and audio content.\n",
    "    \n",
    "    Combines image generation, audio generation, and synchronization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        from diffusers import StableDiffusionPipeline\n",
    "        \n",
    "        # Image generator\n",
    "        self.image_pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            \"runwayml/stable-diffusion-v1-5\"\n",
    "        )\n",
    "        \n",
    "        # Audio generator (placeholder for real API)\n",
    "        self.audio_generator = AudioGeneratorAPI()\n",
    "        \n",
    "    def generate_video(self, text_prompt, duration=10, fps=24):\n",
    "        \"\"\"\n",
    "        Generate video with matching audio from text.\n",
    "        \n",
    "        Args:\n",
    "            text_prompt: Description of desired video\n",
    "            duration: Video length in seconds\n",
    "            fps: Frames per second\n",
    "        \n",
    "        Returns:\n",
    "            video_path: Path to generated video file\n",
    "        \"\"\"\n",
    "        total_frames = duration * fps\n",
    "        \n",
    "        print(f\"Generating {total_frames} frames...\")\n",
    "        \n",
    "        # Generate keyframes\n",
    "        keyframes = []\n",
    "        for i in range(0, total_frames, fps):  # 1 keyframe per second\n",
    "            # Add temporal variation to prompt\n",
    "            frame_prompt = f\"{text_prompt}, frame {i/fps:.1f}s\"\n",
    "            \n",
    "            # Generate image\n",
    "            image = self.image_pipeline(frame_prompt).images[0]\n",
    "            keyframes.append(image)\n",
    "        \n",
    "        # Interpolate between keyframes\n",
    "        all_frames = self.interpolate_frames(keyframes, fps)\n",
    "        \n",
    "        # Generate matching audio\n",
    "        print(\"Generating audio...\")\n",
    "        audio_prompt = self.extract_audio_description(text_prompt)\n",
    "        audio = self.audio_generator.generate(audio_prompt, duration=duration)\n",
    "        \n",
    "        # Combine into video\n",
    "        video_path = self.create_video(all_frames, audio, fps)\n",
    "        \n",
    "        return video_path\n",
    "    \n",
    "    def interpolate_frames(self, keyframes, target_fps):\n",
    "        \"\"\"Smooth interpolation between keyframes\"\"\"\n",
    "        from PIL import Image\n",
    "        import numpy as np\n",
    "        \n",
    "        all_frames = []\n",
    "        for i in range(len(keyframes) - 1):\n",
    "            frame1 = np.array(keyframes[i])\n",
    "            frame2 = np.array(keyframes[i + 1])\n",
    "            \n",
    "            # Linear interpolation\n",
    "            for alpha in np.linspace(0, 1, target_fps):\n",
    "                interpolated = (1 - alpha) * frame1 + alpha * frame2\n",
    "                all_frames.append(Image.fromarray(interpolated.astype('uint8')))\n",
    "        \n",
    "        return all_frames\n",
    "    \n",
    "    def extract_audio_description(self, video_prompt):\n",
    "        \"\"\"Convert video description to audio description\"\"\"\n",
    "        # Map visual to audio\n",
    "        mapping = {\n",
    "            'ocean': 'sound of waves, seagulls, gentle wind',\n",
    "            'city': 'urban sounds, traffic, people talking',\n",
    "            'forest': 'birds chirping, rustling leaves, wind',\n",
    "            'cafe': 'background chatter, coffee machine, soft music'\n",
    "        }\n",
    "        \n",
    "        # Simple keyword matching\n",
    "        for keyword, audio_desc in mapping.items():\n",
    "            if keyword in video_prompt.lower():\n",
    "                return audio_desc\n",
    "        \n",
    "        return 'ambient background music'\n",
    "    \n",
    "    def create_video(self, frames, audio, fps):\n",
    "        \"\"\"Combine frames and audio into video file\"\"\"\n",
    "        import moviepy.editor as mp\n",
    "        \n",
    "        # Create video clip from frames\n",
    "        video = mp.ImageSequenceClip([np.array(f) for f in frames], fps=fps)\n",
    "        \n",
    "        # Add audio\n",
    "        audio_clip = mp.AudioFileClip(audio)\n",
    "        video = video.set_audio(audio_clip)\n",
    "        \n",
    "        # Save\n",
    "        output_path = 'generated_video.mp4'\n",
    "        video.write_videofile(output_path, codec='libx264', audio_codec='aac')\n",
    "        \n",
    "        return output_path\n",
    "\n",
    "\n",
    "# Business application: Automated video ads\n",
    "class VideoAdGenerator:\n",
    "    \"\"\"\n",
    "    Generate video advertisements automatically.\n",
    "    \n",
    "    Creates product videos with matching music and voiceover.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.multimodal = MultimodalGenerator()\n",
    "        self.tts = TTSSystem()\n",
    "    \n",
    "    def create_product_ad(self, product_info, style='professional'):\n",
    "        \"\"\"\n",
    "        Create complete video ad for product.\n",
    "        \n",
    "        Args:\n",
    "            product_info: Dict with product details\n",
    "            style: Ad style (professional, energetic, calm)\n",
    "        \n",
    "        Returns:\n",
    "            video_path: Generated video ad\n",
    "        \"\"\"\n",
    "        # Generate script\n",
    "        script = self.generate_script(product_info, style)\n",
    "        \n",
    "        # Generate visuals\n",
    "        visual_prompt = f\"{product_info['name']}, {style} product photography\"\n",
    "        video_frames = self.multimodal.generate_video(visual_prompt, duration=15)\n",
    "        \n",
    "        # Generate voiceover\n",
    "        voiceover = self.tts.synthesize(script)\n",
    "        \n",
    "        # Generate background music\n",
    "        music_style = {\n",
    "            'professional': 'corporate uplifting',\n",
    "            'energetic': 'upbeat electronic',\n",
    "            'calm': 'soft ambient'\n",
    "        }[style]\n",
    "        music = generate_music(music_style, duration=15)\n",
    "        \n",
    "        # Mix audio\n",
    "        final_audio = self.mix_audio(voiceover, music)\n",
    "        \n",
    "        # Combine\n",
    "        final_video = self.combine_video_audio(video_frames, final_audio)\n",
    "        \n",
    "        return final_video\n",
    "    \n",
    "    def generate_script(self, product_info, style):\n",
    "        \"\"\"Generate ad script\"\"\"\n",
    "        templates = {\n",
    "            'professional': f\"Introducing {product_info['name']}. {product_info['description']}. Available now.\",\n",
    "            'energetic': f\"Get ready for {product_info['name']}! {product_info['description']}! Get yours today!\",\n",
    "            'calm': f\"Experience {product_info['name']}. {product_info['description']}. Discover more.\"\n",
    "        }\n",
    "        return templates[style]\n",
    "\n",
    "\n",
    "# ROI calculation\n",
    "def video_ad_roi():\n",
    "    \"\"\"Calculate ROI for automated video ads\"\"\"\n",
    "    traditional = {\n",
    "        'videographer': 2000,\n",
    "        'voice_actor': 500,\n",
    "        'music_licensing': 300,\n",
    "        'editing': 1000,\n",
    "        'total': 3800,\n",
    "        'time': '1 week'\n",
    "    }\n",
    "    \n",
    "    ai_approach = {\n",
    "        'api_costs': 100,\n",
    "        'review_editing': 200,\n",
    "        'total': 300,\n",
    "        'time': '1 hour'\n",
    "    }\n",
    "    \n",
    "    savings = traditional['total'] - ai_approach['total']\n",
    "    \n",
    "    print(f\"Savings per ad: ${savings} ({savings/traditional['total']*100:.1f}% reduction)\")\n",
    "    print(f\"For 50 product ads: ${savings * 50:,} saved\")\n",
    "    print(f\"Time savings: {traditional['time']} → {ai_approach['time']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_ad_roi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bee033",
   "metadata": {},
   "source": [
    "**Real-World Multimodal Systems:**\n",
    "\n",
    "**1. Video Diffusion Models**\n",
    "- Runway Gen-2\n",
    "- Pika Labs\n",
    "- Stability AI Video\n",
    "\n",
    "**2. Audio-Visual Learning**\n",
    "- CLIP for audio-visual alignment\n",
    "- AudioCLIP\n",
    "- ImageBind (Meta)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26582eba",
   "metadata": {},
   "source": [
    "Real-Time Generation Considerations\n",
    "\n",
    "### From Offline to Interactive\n",
    "\n",
    "**The Real-Time Challenge:**\n",
    "\n",
    "Traditional generation is slow:\n",
    "- Diffusion: 2-5 seconds for one image\n",
    "- Audio: 1-2 seconds for 1 second of audio\n",
    "- Video: Minutes for a few seconds\n",
    "\n",
    "**Business Need:** Real-time interactive applications.\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "**1. Model Distillation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b4f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastDiffusion:\n",
    "    \"\"\"\n",
    "    Distilled diffusion model for real-time generation.\n",
    "    \n",
    "    Student model learns to generate in fewer steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, teacher_model, num_steps=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            teacher_model: Original slow diffusion model\n",
    "            num_steps: Target number of steps (vs 50-1000)\n",
    "        \"\"\"\n",
    "        self.teacher = teacher_model\n",
    "        self.student = self.create_student_model()\n",
    "        self.num_steps = num_steps\n",
    "    \n",
    "    def distill(self, training_data):\n",
    "        \"\"\"\n",
    "        Train student to match teacher's output.\n",
    "        \n",
    "        Teacher generates with 50 steps.\n",
    "        Student learns to match with only 4 steps.\n",
    "        \"\"\"\n",
    "        for data in training_data:\n",
    "            # Teacher prediction (slow, high quality)\n",
    "            with torch.no_grad():\n",
    "                teacher_output = self.teacher(data, num_steps=50)\n",
    "            \n",
    "            # Student prediction (fast)\n",
    "            student_output = self.student(data, num_steps=self.num_steps)\n",
    "            \n",
    "            # Match outputs\n",
    "            loss = F.mse_loss(student_output, teacher_output)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    def generate_realtime(self, prompt):\n",
    "        \"\"\"Generate in <1 second\"\"\"\n",
    "        return self.student(prompt, num_steps=self.num_steps)\n",
    "\n",
    "\n",
    "# Example: SDXL Turbo (Stability AI)\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import torch\n",
    "\n",
    "pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\",\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ")\n",
    "pipeline.to(\"cuda\")\n",
    "\n",
    "# Single-step generation!\n",
    "image = pipeline(\n",
    "    prompt=\"A beautiful sunset\",\n",
    "    num_inference_steps=1,  # Just 1 step!\n",
    "    guidance_scale=0.0  # No guidance needed\n",
    ").images[0]\n",
    "\n",
    "# Result: ~0.1 seconds per image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be7b96e",
   "metadata": {},
   "source": [
    "**2. Latent Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf13f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CachedGenerator:\n",
    "    \"\"\"\n",
    "    Cache intermediate results for faster generation.\n",
    "    \n",
    "    Useful for interactive editing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "    \n",
    "    def generate_with_cache(self, base_prompt, modifications):\n",
    "        \"\"\"\n",
    "        Generate variations quickly by caching base.\n",
    "        \n",
    "        Args:\n",
    "            base_prompt: Base description\n",
    "            modifications: List of edits\n",
    "        \n",
    "        Returns:\n",
    "            images: Generated variations\n",
    "        \"\"\"\n",
    "        # Generate base once\n",
    "        cache_key = base_prompt\n",
    "        if cache_key not in self.cache:\n",
    "            self.cache[cache_key] = self.generate_base(base_prompt)\n",
    "        \n",
    "        base_latent = self.cache[cache_key]\n",
    "        \n",
    "        # Apply modifications quickly\n",
    "        results = []\n",
    "        for mod in modifications:\n",
    "            # Start from cached latent\n",
    "            modified = self.apply_modification(base_latent, mod)\n",
    "            results.append(modified)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f462a2",
   "metadata": {},
   "source": [
    "**3. Progressive Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef7e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressive_generation(prompt):\n",
    "    \"\"\"\n",
    "    Generate low-res quickly, then refine.\n",
    "    \n",
    "    Show user something immediately, improve over time.\n",
    "    \"\"\"\n",
    "    # Quick low-res preview (0.1s)\n",
    "    preview = generate_image(prompt, resolution=128, steps=4)\n",
    "    display(preview)\n",
    "    \n",
    "    # Medium quality (0.5s)\n",
    "    medium = generate_image(prompt, resolution=256, steps=8)\n",
    "    display(medium)\n",
    "    \n",
    "    # Final quality (2s)\n",
    "    final = generate_image(prompt, resolution=512, steps=20)\n",
    "    display(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784532a",
   "metadata": {},
   "source": [
    "**Real-Time Applications:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58047ef",
   "metadata": {},
   "source": [
    "**Performance Targets:**\n",
    "\n",
    "| Application | Latency Target | Solution |\n",
    "|-------------|---------------|----------|\n",
    "| Image Editor | <100ms | Distilled models + caching |\n",
    "| Video Chat Effects | <33ms (30 FPS) | Lightweight models |\n",
    "| Gaming | <16ms (60 FPS) | Pre-generated + blending |\n",
    "| Music Apps | <10ms | Streaming generation |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66605b29",
   "metadata": {},
   "source": [
    "Quality Metrics and Evaluation\n",
    "\n",
    "### How Do We Know If It's Good?\n",
    "\n",
    "**The Challenge:** Evaluating generative models is subjective.\n",
    "\n",
    "**Evaluation Approaches:**\n",
    "\n",
    "**1. Objective Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d11ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeMetrics:\n",
    "    \"\"\"\n",
    "    Compute objective quality metrics for generated content.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "        from torchmetrics.image.inception import InceptionScore\n",
    "        \n",
    "        self.fid = FrechetInceptionDistance(feature=2048)\n",
    "        self.inception_score = InceptionScore()\n",
    "    \n",
    "    def compute_image_quality(self, generated_images, real_images):\n",
    "        \"\"\"\n",
    "        Compute FID and Inception Score.\n",
    "        \n",
    "        Args:\n",
    "            generated_images: Generated images\n",
    "            real_images: Real reference images\n",
    "        \n",
    "        Returns:\n",
    "            metrics: Dict of quality scores\n",
    "        \"\"\"\n",
    "        # Fréchet Inception Distance (FID)\n",
    "        # Lower is better, measures distribution similarity\n",
    "        fid_score = self.fid(generated_images, real_images)\n",
    "        \n",
    "        # Inception Score (IS)\n",
    "        # Higher is better, measures quality and diversity\n",
    "        is_score, _ = self.inception_score(generated_images)\n",
    "        \n",
    "        return {\n",
    "            'fid': fid_score.item(),\n",
    "            'inception_score': is_score.item()\n",
    "        }\n",
    "    \n",
    "    def compute_audio_quality(self, generated_audio, reference_audio):\n",
    "        \"\"\"\n",
    "        Audio quality metrics.\n",
    "        \"\"\"\n",
    "        import librosa\n",
    "        \n",
    "        # Mel Cepstral Distortion (MCD)\n",
    "        # Lower is better\n",
    "        mcd = self.compute_mcd(generated_audio, reference_audio)\n",
    "        \n",
    "        # Signal-to-Noise Ratio\n",
    "        snr = self.compute_snr(generated_audio, reference_audio)\n",
    "        \n",
    "        # Perceptual Evaluation of Speech Quality (PESQ)\n",
    "        from pesq import pesq\n",
    "        pesq_score = pesq(16000, reference_audio, generated_audio, 'wb')\n",
    "        \n",
    "        return {\n",
    "            'mcd': mcd,\n",
    "            'snr': snr,\n",
    "            'pesq': pesq_score\n",
    "        }\n",
    "    \n",
    "    def compute_music_quality(self, generated_music):\n",
    "        \"\"\"Music-specific metrics\"\"\"\n",
    "        # Pitch accuracy\n",
    "        pitch_acc = self.evaluate_pitch(generated_music)\n",
    "        \n",
    "        # Rhythm consistency\n",
    "        rhythm_score = self.evaluate_rhythm(generated_music)\n",
    "        \n",
    "        # Harmonic coherence\n",
    "        harmony_score = self.evaluate_harmony(generated_music)\n",
    "        \n",
    "        return {\n",
    "            'pitch_accuracy': pitch_acc,\n",
    "            'rhythm_consistency': rhythm_score,\n",
    "            'harmonic_coherence': harmony_score\n",
    "        }\n",
    "\n",
    "\n",
    "# Human evaluation\n",
    "class HumanEvaluation:\n",
    "    \"\"\"\n",
    "    Conduct human evaluation studies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def run_ab_test(self, generated_samples, real_samples):\n",
    "        \"\"\"\n",
    "        A/B test: Can humans tell the difference?\n",
    "        \n",
    "        Returns:\n",
    "            fooling_rate: % of time AI fools humans\n",
    "        \"\"\"\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for gen, real in zip(generated_samples, real_samples):\n",
    "            # Show pair to human, ask which is real\n",
    "            if random.random() < 0.5:\n",
    "                shown = [(gen, 'A'), (real, 'B')]\n",
    "            else:\n",
    "                shown = [(real, 'A'), (gen, 'B')]\n",
    "            \n",
    "            answer = get_human_judgment(shown)\n",
    "            if answer == 'B' if shown[1][1] == 'real' else 'A':\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        \n",
    "        fooling_rate = 1 - (correct / total)\n",
    "        return fooling_rate\n",
    "    \n",
    "    def collect_ratings(self, samples, criteria=['quality', 'creativity', 'relevance']):\n",
    "        \"\"\"\n",
    "        Collect Likert scale ratings.\n",
    "        \n",
    "        Args:\n",
    "            samples: Generated samples\n",
    "            criteria: What to rate\n",
    "        \n",
    "        Returns:\n",
    "            ratings: Mean ratings per criterion\n",
    "        \"\"\"\n",
    "        ratings = {c: [] for c in criteria}\n",
    "        \n",
    "        for sample in samples:\n",
    "            for criterion in criteria:\n",
    "                rating = get_human_rating(sample, criterion, scale=1-5)\n",
    "                ratings[criterion].append(rating)\n",
    "        \n",
    "        # Compute means\n",
    "        mean_ratings = {c: np.mean(ratings[c]) for c in criteria}\n",
    "        \n",
    "        return mean_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f1de27",
   "metadata": {},
   "source": [
    "**2. Task-Specific Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cbe4c4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4accf206",
   "metadata": {},
   "source": [
    "Compression and Efficiency\n",
    "\n",
    "### Making GenAI Practical at Scale\n",
    "\n",
    "**The Cost Problem:**\n",
    "\n",
    "Large models are expensive:\n",
    "- Stable Diffusion: 860M parameters\n",
    "- DALL-E 2: 3.5B parameters\n",
    "- MusicLM: Multiple large models\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "**1. Model Quantization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6543b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def quantize_model(model, bits=8):\n",
    "    \"\"\"\n",
    "    Reduce model precision.\n",
    "    \n",
    "    FP32 (32-bit) → INT8 (8-bit)\n",
    "    4x smaller, 2-4x faster\n",
    "    \"\"\"\n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model,\n",
    "        {torch.nn.Linear, torch.nn.Conv2d},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "\n",
    "# Example: INT8 quantization\n",
    "model_fp32 = load_model()  # 3.4 GB\n",
    "model_int8 = quantize_model(model_fp32)  # 850 MB\n",
    "\n",
    "# Minimal quality loss, major speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c19a2",
   "metadata": {},
   "source": [
    "**2. Pruning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d816a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_model(model, sparsity=0.5):\n",
    "    \"\"\"\n",
    "    Remove unimportant weights.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network\n",
    "        sparsity: Fraction of weights to remove\n",
    "    \"\"\"\n",
    "    import torch.nn.utils.prune as prune\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            prune.l1_unstructured(module, name='weight', amount=sparsity)\n",
    "            prune.remove(module, 'weight')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# 50% sparsity → 50% fewer parameters → 2x faster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb6a05",
   "metadata": {},
   "source": [
    "**3. Knowledge Distillation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8565ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distill_model(large_model, small_model, data):\n",
    "    \"\"\"\n",
    "    Train small model to mimic large model.\n",
    "    \n",
    "    Student learns from teacher's outputs.\n",
    "    \"\"\"\n",
    "    temperature = 3.0\n",
    "    \n",
    "    for batch in data:\n",
    "        # Teacher predictions (soft targets)\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = large_model(batch)\n",
    "            soft_targets = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "        \n",
    "        # Student predictions\n",
    "        student_logits = small_model(batch)\n",
    "        student_probs = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "        \n",
    "        # Distillation loss\n",
    "        loss = F.kl_div(student_probs, soft_targets, reduction='batchmean')\n",
    "        loss = loss * (temperature ** 2)\n",
    "        \n",
    "        # Train student\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return small_model\n",
    "\n",
    "\n",
    "# Result: 10x smaller model, 90% of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47cb26",
   "metadata": {},
   "source": [
    "**4. Efficient Architectures**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a5aab",
   "metadata": {},
   "source": [
    "**Cost Optimization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7db256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_deployment(model_type, expected_requests_per_day):\n",
    "    \"\"\"\n",
    "    Choose optimal deployment strategy.\n",
    "    \n",
    "    Args:\n",
    "        model_type: 'image', 'audio', or 'music'\n",
    "        expected_requests_per_day: Daily request volume\n",
    "    \n",
    "    Returns:\n",
    "        recommendation: Deployment strategy\n",
    "    \"\"\"\n",
    "    strategies = {\n",
    "        'low': {  # <1000 requests/day\n",
    "            'approach': 'API service (pay-per-use)',\n",
    "            'cost': expected_requests_per_day * 0.01,\n",
    "            'latency': 'medium'\n",
    "        },\n",
    "        'medium': {  # 1000-10000 requests/day\n",
    "            'approach': 'Shared GPU instance',\n",
    "            'cost': 500,  # monthly\n",
    "            'latency': 'low'\n",
    "        },\n",
    "        'high': {  # >10000 requests/day\n",
    "            'approach': 'Dedicated GPU cluster + quantization',\n",
    "            'cost': 2000,  # monthly\n",
    "            'latency': 'very low'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if expected_requests_per_day < 1000:\n",
    "        return strategies['low']\n",
    "    elif expected_requests_per_day < 10000:\n",
    "        return strategies['medium']\n",
    "    else:\n",
    "        return strategies['high']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15a379",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd678806",
   "metadata": {},
   "source": [
    "Break & Review - Mid-Week Checkpoint\n",
    "\n",
    "### What We've Covered So Far\n",
    "\n",
    "**Part 1: Image Generation (Slides 1-15)**\n",
    "✅ VAEs - Smooth latent spaces\n",
    "✅ GANs - Adversarial training\n",
    "✅ Diffusion Models - State-of-the-art\n",
    "✅ Stable Diffusion - Text-to-image\n",
    "✅ ControlNet - Precise control\n",
    "✅ Image editing - Inpainting, style transfer\n",
    "\n",
    "**Part 2: Audio & Music (Slides 16-25)**\n",
    "✅ Audio representations\n",
    "✅ WaveNet - Sample-by-sample generation\n",
    "✅ TTS - Text-to-speech systems\n",
    "✅ Voice cloning\n",
    "✅ Music RNN & Transformers\n",
    "✅ MuseNet & MusicLM\n",
    "✅ Music style transfer\n",
    "\n",
    "**Part 3: Advanced Topics (Slides 26-30)**\n",
    "✅ Multimodal generation\n",
    "✅ Real-time considerations\n",
    "✅ Quality metrics\n",
    "✅ Compression & efficiency\n",
    "✅ Deployment strategies\n",
    "\n",
    "---\n",
    "\n",
    "**Quick Quiz:**\n",
    "\n",
    "1. **What's the main advantage of diffusion models over GANs?**\n",
    "   - Answer: More stable training, no mode collapse, better quality\n",
    "\n",
    "2. **Why is WaveNet slow for audio generation?**\n",
    "   - Answer: Autoregressive, generates sample-by-sample (44,100/second)\n",
    "\n",
    "3. **What's the key to real-time generation?**\n",
    "   - Answer: Distillation, quantization, efficient architectures\n",
    "\n",
    "4. **How do we evaluate if generated content is good?**\n",
    "   - Answer: Combination of objective metrics (FID, IS) and human evaluation\n",
    "\n",
    "5. **What's the cost-performance trade-off?**\n",
    "   - Answer: Smaller models = faster/cheaper but slightly lower quality\n",
    "\n",
    "---\n",
    "\n",
    "**Break Activity (10 minutes):**\n",
    "\n",
    "Try these demos:\n",
    "1. Generate an image: https://huggingface.co/spaces/stabilityai/stable-diffusion\n",
    "2. Generate music: https://google-research.github.io/seanet/musiclm/examples/\n",
    "3. Try voice cloning: https://elevenlabs.io\n",
    "\n",
    "---\n",
    "\n",
    "**Coming Up (Slides 31-40):**\n",
    "- Real-world business applications\n",
    "- Industry case studies\n",
    "- ROI calculators\n",
    "- Implementation strategies\n",
    "- Best practices\n",
    "- Ethics and governance\n",
    "- Week 5 assignment\n",
    "\n",
    "**Take a 10-minute break! ☕**\n",
    "\n",
    "---\n",
    "\n",
    "**End of Batch 5 (Slides 26-30)**\n",
    "\n",
    "*Continue to Batch 6 for Business Applications Part 1 (Slides 31-35)*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
