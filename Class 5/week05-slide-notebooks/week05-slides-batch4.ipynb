{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b7659f",
   "metadata": {},
   "source": [
    "# Week05 Slides Batch4\n",
    "\n",
    "**Interactive Jupyter Notebook Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca979a6d",
   "metadata": {},
   "source": [
    "# Week 5: Image Generation, Audio, and Music - Slides Batch 4 (Slides 21-25)\n",
    "\n",
    "**Course:** BUAN 6v99.SW2 - Generative AI for Business  \n",
    "**Continuation from Batch 3**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e52a3",
   "metadata": {},
   "source": [
    "Music Generation - Special Challenges\n",
    "\n",
    "### Music vs Speech - What Makes Music Different?\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Aspect | Speech | Music |\n",
    "|--------|--------|-------|\n",
    "| **Structure** | Sequential, linear | Harmonic, polyphonic |\n",
    "| **Elements** | Phonemes, words | Notes, chords, rhythm |\n",
    "| **Complexity** | 1 voice | Multiple instruments |\n",
    "| **Duration** | Seconds to minutes | Minutes to hours |\n",
    "| **Evaluation** | Intelligibility | Aesthetic quality |\n",
    "\n",
    "**Music Representations:**\n",
    "\n",
    "**1. Audio Waveform**\n",
    "- Raw audio signal\n",
    "- Pros: Complete information\n",
    "- Cons: Very high-dimensional, hard to edit\n",
    "\n",
    "**2. MIDI (Symbolic)**\n",
    "- Musical notes with timing and velocity\n",
    "- Pros: Compact, editable, interpretable\n",
    "- Cons: No audio timbre information\n",
    "\n",
    "**3. Piano Roll**\n",
    "- Visual representation of MIDI\n",
    "- Time × Pitch grid\n",
    "- Pros: Easy to visualize and edit\n",
    "- Cons: Limited to pitch information\n",
    "\n",
    "**4. Spectrogram**\n",
    "- Time-frequency representation\n",
    "- Pros: Works for any audio\n",
    "- Cons: Loses some structure\n",
    "\n",
    "**Music Generation Approaches:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d8cd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MusicRepresentation:\n",
    "    \"\"\"\n",
    "    Handle different music representations.\n",
    "    \n",
    "    Convert between MIDI, piano roll, and audio.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fs=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fs: Frames per second for piano roll\n",
    "        \"\"\"\n",
    "        self.fs = fs\n",
    "    \n",
    "    def midi_to_piano_roll(self, midi_file):\n",
    "        \"\"\"\n",
    "        Convert MIDI to piano roll representation.\n",
    "        \n",
    "        Args:\n",
    "            midi_file: Path to MIDI file\n",
    "        \n",
    "        Returns:\n",
    "            piano_roll: (128, time_steps) array\n",
    "        \"\"\"\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "        piano_roll = midi_data.get_piano_roll(fs=self.fs)\n",
    "        return piano_roll\n",
    "    \n",
    "    def piano_roll_to_midi(self, piano_roll, program=0):\n",
    "        \"\"\"\n",
    "        Convert piano roll back to MIDI.\n",
    "        \n",
    "        Args:\n",
    "            piano_roll: (128, time_steps) array\n",
    "            program: MIDI instrument program\n",
    "        \n",
    "        Returns:\n",
    "            midi_data: PrettyMIDI object\n",
    "        \"\"\"\n",
    "        midi_data = pretty_midi.PrettyMIDI()\n",
    "        instrument = pretty_midi.Instrument(program=program)\n",
    "        \n",
    "        # Find note events\n",
    "        for pitch in range(128):\n",
    "            note_changes = np.diff(piano_roll[pitch] > 0, prepend=0, append=0)\n",
    "            note_on = np.where(note_changes == 1)[0]\n",
    "            note_off = np.where(note_changes == -1)[0]\n",
    "            \n",
    "            for start, end in zip(note_on, note_off):\n",
    "                note = pretty_midi.Note(\n",
    "                    velocity=100,\n",
    "                    pitch=pitch,\n",
    "                    start=start / self.fs,\n",
    "                    end=end / self.fs\n",
    "                )\n",
    "                instrument.notes.append(note)\n",
    "        \n",
    "        midi_data.instruments.append(instrument)\n",
    "        return midi_data\n",
    "    \n",
    "    def visualize_piano_roll(self, piano_roll, title=\"Piano Roll\"):\n",
    "        \"\"\"Visualize piano roll\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(piano_roll, aspect='auto', origin='lower', cmap='Blues')\n",
    "        plt.xlabel('Time (frames)')\n",
    "        plt.ylabel('Pitch')\n",
    "        plt.title(title)\n",
    "        plt.colorbar(label='Velocity')\n",
    "        plt.tight_layout()\n",
    "        return plt.gcf()\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    music_repr = MusicRepresentation(fs=100)\n",
    "    \n",
    "    # Load MIDI\n",
    "    piano_roll = music_repr.midi_to_piano_roll('sample.mid')\n",
    "    print(f\"Piano roll shape: {piano_roll.shape}\")\n",
    "    print(f\"Duration: {piano_roll.shape[1] / 100:.2f} seconds\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig = music_repr.visualize_piano_roll(piano_roll)\n",
    "    plt.savefig('piano_roll.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Convert back\n",
    "    midi_out = music_repr.piano_roll_to_midi(piano_roll)\n",
    "    midi_out.write('reconstructed.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e533fa29",
   "metadata": {},
   "source": [
    "**Business Relevance:**\n",
    "- Background music for videos\n",
    "- Game soundtracks\n",
    "- Retail ambiance\n",
    "- Personalized playlists\n",
    "- Music therapy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2bede6",
   "metadata": {},
   "source": [
    "Music RNN - Simple Melody Generation\n",
    "\n",
    "### Generating Melodies with Recurrent Networks\n",
    "\n",
    "**Approach:** Treat music as a sequence, like text generation.\n",
    "\n",
    "**Music RNN Architecture:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a8a76e",
   "metadata": {},
   "source": [
    "```python\n",
    "Previous Notes → LSTM → Next Note Distribution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bec12",
   "metadata": {},
   "source": [
    "**Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20aa93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MusicLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM for melody generation.\n",
    "    \n",
    "    Predicts next note given previous notes.\n",
    "    Similar to language model but for music.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=128, embedding_dim=128, hidden_dim=512, num_layers=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Number of possible notes (88 for piano, 128 for MIDI)\n",
    "            embedding_dim: Note embedding dimension\n",
    "            hidden_dim: LSTM hidden dimension\n",
    "            num_layers: Number of LSTM layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Note embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Note sequence (batch, seq_len)\n",
    "            hidden: Optional hidden state\n",
    "        \n",
    "        Returns:\n",
    "            logits: Next note predictions\n",
    "            hidden: Updated hidden state\n",
    "        \"\"\"\n",
    "        # Embed notes\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # LSTM\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.fc(output)\n",
    "        \n",
    "        return logits, hidden\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, start_sequence, length=100, temperature=1.0):\n",
    "        \"\"\"\n",
    "        Generate melody autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            start_sequence: Seed notes (batch, seq_len)\n",
    "            length: Number of notes to generate\n",
    "            temperature: Sampling temperature\n",
    "        \n",
    "        Returns:\n",
    "            generated: Generated note sequence\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        generated = start_sequence\n",
    "        hidden = None\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # Predict next note\n",
    "            logits, hidden = self.forward(generated[:, -1:], hidden)\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits[:, -1, :] / temperature, dim=-1)\n",
    "            next_note = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # Append\n",
    "            generated = torch.cat([generated, next_note], dim=1)\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "def train_music_lstm(model, sequences, epochs=10):\n",
    "    \"\"\"\n",
    "    Train music generation model.\n",
    "    \n",
    "    Args:\n",
    "        model: MusicLSTM model\n",
    "        sequences: Training sequences (list of note sequences)\n",
    "        epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"TRAINING MUSIC LSTM\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            # Prepare input and target\n",
    "            x = sequence[:-1]\n",
    "            y = sequence[1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            logits, _ = model(x.unsqueeze(0))\n",
    "            loss = criterion(logits.reshape(-1, model.vocab_size), y)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(sequences)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Complete example\n",
    "if __name__ == \"__main__\":\n",
    "    # Create model\n",
    "    model = MusicLSTM(vocab_size=88, embedding_dim=128, hidden_dim=512)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Load training data (simplified)\n",
    "    sequences = load_midi_sequences('music_dataset/')\n",
    "    \n",
    "    # Train\n",
    "    model = train_music_lstm(model, sequences, epochs=20)\n",
    "    \n",
    "    # Generate new melody\n",
    "    print(\"\\nGenerating new melody...\")\n",
    "    start = torch.tensor([[60, 62, 64]])  # C, D, E\n",
    "    generated = model.generate(start, length=100, temperature=1.0)\n",
    "    \n",
    "    # Convert to MIDI\n",
    "    notes_to_midi(generated[0], 'generated_melody.mid')\n",
    "    print(\"✓ Melody saved to 'generated_melody.mid'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da75c95b",
   "metadata": {},
   "source": [
    "**Magenta (Google's Music Generation)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a12c080",
   "metadata": {},
   "source": [
    "**Limitations:**\n",
    "- Single melody line (monophonic)\n",
    "- Limited long-term structure\n",
    "- No harmonic awareness\n",
    "- Repetitive patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a95b4",
   "metadata": {},
   "source": [
    "Music Transformer - Polyphonic Generation\n",
    "\n",
    "### Multiple Instruments and Harmony\n",
    "\n",
    "**Music Transformer:** Apply transformer architecture to music generation.\n",
    "\n",
    "**Key Innovation:** Self-attention for capturing musical relationships.\n",
    "\n",
    "**Architecture:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3356fc",
   "metadata": {},
   "source": [
    "```python\n",
    "MIDI Events → Token Embedding → Positional Encoding\n",
    "                                        ↓\n",
    "                                  Transformer Blocks\n",
    "                                        ↓\n",
    "                                  Next Event Prediction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26753a9",
   "metadata": {},
   "source": [
    "**Music Tokenization:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e09cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicTokenizer:\n",
    "    \"\"\"\n",
    "    Tokenize music for transformer input.\n",
    "    \n",
    "    Represents MIDI events as discrete tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Token types\n",
    "        self.note_on_offset = 0\n",
    "        self.note_off_offset = 128\n",
    "        self.time_shift_offset = 256\n",
    "        self.velocity_offset = 356\n",
    "        \n",
    "        self.vocab_size = 456  # Total tokens\n",
    "    \n",
    "    def encode_event(self, event_type, value):\n",
    "        \"\"\"\n",
    "        Encode MIDI event as token.\n",
    "        \n",
    "        Args:\n",
    "            event_type: 'note_on', 'note_off', 'time_shift', 'velocity'\n",
    "            value: Event value\n",
    "        \n",
    "        Returns:\n",
    "            token: Integer token\n",
    "        \"\"\"\n",
    "        if event_type == 'note_on':\n",
    "            return self.note_on_offset + value\n",
    "        elif event_type == 'note_off':\n",
    "            return self.note_off_offset + value\n",
    "        elif event_type == 'time_shift':\n",
    "            return self.time_shift_offset + min(value, 100)\n",
    "        elif event_type == 'velocity':\n",
    "            return self.velocity_offset + min(value // 4, 32)\n",
    "    \n",
    "    def decode_token(self, token):\n",
    "        \"\"\"Decode token back to MIDI event\"\"\"\n",
    "        if token < 128:\n",
    "            return ('note_on', token)\n",
    "        elif token < 256:\n",
    "            return ('note_off', token - 128)\n",
    "        elif token < 356:\n",
    "            return ('time_shift', token - 256)\n",
    "        else:\n",
    "            return ('velocity', (token - 356) * 4)\n",
    "    \n",
    "    def midi_to_tokens(self, midi_file):\n",
    "        \"\"\"Convert MIDI file to token sequence\"\"\"\n",
    "        midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "        tokens = []\n",
    "        \n",
    "        # Sort all events by time\n",
    "        events = []\n",
    "        for instrument in midi_data.instruments:\n",
    "            for note in instrument.notes:\n",
    "                events.append(('note_on', note.start, note.pitch, note.velocity))\n",
    "                events.append(('note_off', note.end, note.pitch, 0))\n",
    "        \n",
    "        events.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # Convert to tokens\n",
    "        current_time = 0\n",
    "        for event_type, time, pitch, velocity in events:\n",
    "            # Time shift\n",
    "            time_diff = int((time - current_time) * 100)\n",
    "            if time_diff > 0:\n",
    "                tokens.append(self.encode_event('time_shift', time_diff))\n",
    "            \n",
    "            # Velocity (for note_on)\n",
    "            if event_type == 'note_on':\n",
    "                tokens.append(self.encode_event('velocity', velocity))\n",
    "            \n",
    "            # Note event\n",
    "            tokens.append(self.encode_event(event_type, pitch))\n",
    "            \n",
    "            current_time = time\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for polyphonic music generation.\n",
    "    \n",
    "    Handles multiple instruments and complex harmonies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=456, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(10000, d_model))\n",
    "        \n",
    "        # Transformer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Output\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Token sequence (batch, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            logits: Next token predictions\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        # Embed and add position\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded + self.pos_encoder[:seq_len]\n",
    "        \n",
    "        # Create causal mask\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(x.device)\n",
    "        \n",
    "        # Transformer\n",
    "        output = self.transformer(embedded, mask=mask, is_causal=True)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.fc_out(output)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, start_tokens, length=1000, temperature=1.0, top_k=40):\n",
    "        \"\"\"\n",
    "        Generate music autoregressively.\n",
    "        \n",
    "        Args:\n",
    "            start_tokens: Seed tokens\n",
    "            length: Number of tokens to generate\n",
    "            temperature: Sampling temperature\n",
    "            top_k: Top-k sampling\n",
    "        \n",
    "        Returns:\n",
    "            generated: Generated token sequence\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        generated = start_tokens\n",
    "        \n",
    "        for _ in range(length):\n",
    "            # Predict next token\n",
    "            logits = self.forward(generated)[:, -1, :]\n",
    "            \n",
    "            # Temperature and top-k sampling\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Top-k filtering\n",
    "            if top_k > 0:\n",
    "                indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            # Sample\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # Append\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# Training and generation\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"MUSIC TRANSFORMER\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize\n",
    "    tokenizer = MusicTokenizer()\n",
    "    model = MusicTransformer(vocab_size=tokenizer.vocab_size)\n",
    "    \n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    midi_files = glob.glob('midi_dataset/*.mid')\n",
    "    token_sequences = [tokenizer.midi_to_tokens(f) for f in midi_files]\n",
    "    \n",
    "    print(f\"Loaded {len(token_sequences)} MIDI files\")\n",
    "    \n",
    "    # Train (simplified)\n",
    "    train_music_transformer(model, token_sequences, epochs=20)\n",
    "    \n",
    "    # Generate new composition\n",
    "    print(\"\\nGenerating new music...\")\n",
    "    start_tokens = torch.tensor([[0, 256, 360, 60]])  # Simple start\n",
    "    generated_tokens = model.generate(start_tokens, length=1000, temperature=1.0)\n",
    "    \n",
    "    # Convert back to MIDI\n",
    "    midi_out = tokenizer.tokens_to_midi(generated_tokens[0])\n",
    "    midi_out.write('generated_composition.mid')\n",
    "    \n",
    "    print(\"✓ Composition saved to 'generated_composition.mid'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754f15d",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "- ✅ Polyphonic (multiple notes simultaneously)\n",
    "- ✅ Long-range dependencies\n",
    "- ✅ Harmonically aware\n",
    "- ✅ Multiple instruments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc02b9",
   "metadata": {},
   "source": [
    "Modern Music Generation - MuseNet & MusicLM\n",
    "\n",
    "### State-of-the-Art Music AI\n",
    "\n",
    "**MuseNet (OpenAI, 2019)**\n",
    "\n",
    "Large-scale transformer trained on MIDI data.\n",
    "\n",
    "**Capabilities:**\n",
    "- 4-minute compositions\n",
    "- 10 different instruments\n",
    "- Multiple genres (classical, jazz, pop, etc.)\n",
    "- Can continue any music snippet\n",
    "\n",
    "**Architecture:**\n",
    "- 72-layer transformer\n",
    "- 24 attention heads\n",
    "- Trained on millions of MIDI files\n",
    "\n",
    "**Usage (API):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae1b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "def generate_with_musenet(prompt, genre='classical', instruments=['piano']):\n",
    "    \"\"\"\n",
    "    Generate music with MuseNet API.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting MIDI sequence\n",
    "        genre: Music genre\n",
    "        instruments: List of instruments\n",
    "    \n",
    "    Returns:\n",
    "        audio: Generated audio\n",
    "    \"\"\"\n",
    "    response = openai.MuseNet.create(\n",
    "        prompt=prompt,\n",
    "        genre=genre,\n",
    "        instruments=instruments,\n",
    "        length=240  # seconds\n",
    "    )\n",
    "    \n",
    "    return response['audio']\n",
    "\n",
    "\n",
    "# Example\n",
    "prompt_midi = load_midi('seed.mid')\n",
    "music = generate_with_musenet(\n",
    "    prompt=prompt_midi,\n",
    "    genre='jazz',\n",
    "    instruments=['piano', 'bass', 'drums']\n",
    ")\n",
    "save_audio(music, 'jazz_composition.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5513469d",
   "metadata": {},
   "source": [
    "**MusicLM (Google, 2023)**\n",
    "\n",
    "Text-to-music generation - like DALL-E but for music!\n",
    "\n",
    "**Key Innovation:** Generate music from text descriptions.\n",
    "\n",
    "**Architecture:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea0779",
   "metadata": {},
   "source": [
    "```python\n",
    "Text Description → Text Encoder (BERT)\n",
    "                         ↓\n",
    "                   Conditioning Vector\n",
    "                         ↓\n",
    "Audio Tokens ← AudioLM (music language model)\n",
    "                         ↓\n",
    "                    Waveform\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2916aa",
   "metadata": {},
   "source": [
    "**Examples:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e2a39e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe067f",
   "metadata": {},
   "source": [
    "Music Style Transfer & Applications\n",
    "\n",
    "### Transforming Musical Styles\n",
    "\n",
    "**Music Style Transfer:** Convert music from one style to another while preserving melody.\n",
    "\n",
    "**Examples:**\n",
    "- Classical → Jazz\n",
    "- Pop → Rock\n",
    "- Piano → Orchestra\n",
    "\n",
    "**Approach 1: CycleGAN for Music**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicStyleGAN:\n",
    "    \"\"\"\n",
    "    Style transfer for music using CycleGAN approach.\n",
    "    \n",
    "    Learns to translate between two musical styles.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, style_a='classical', style_b='jazz'):\n",
    "        self.style_a = style_a\n",
    "        self.style_b = style_b\n",
    "        \n",
    "        # Generators: A→B and B→A\n",
    "        self.g_ab = MusicGenerator()\n",
    "        self.g_ba = MusicGenerator()\n",
    "        \n",
    "        # Discriminators\n",
    "        self.d_a = MusicDiscriminator()\n",
    "        self.d_b = MusicDiscriminator()\n",
    "    \n",
    "    def transfer_style(self, audio, source_style, target_style):\n",
    "        \"\"\"\n",
    "        Transfer music from source to target style.\n",
    "        \n",
    "        Args:\n",
    "            audio: Input audio\n",
    "            source_style: Original style\n",
    "            target_style: Desired style\n",
    "        \n",
    "        Returns:\n",
    "            transformed: Audio in target style\n",
    "        \"\"\"\n",
    "        # Convert to spectrogram\n",
    "        spec = audio_to_spectrogram(audio)\n",
    "        \n",
    "        # Apply style transfer\n",
    "        if source_style == self.style_a and target_style == self.style_b:\n",
    "            spec_transformed = self.g_ab(spec)\n",
    "        else:\n",
    "            spec_transformed = self.g_ba(spec)\n",
    "        \n",
    "        # Convert back to audio\n",
    "        audio_transformed = spectrogram_to_audio(spec_transformed)\n",
    "        \n",
    "        return audio_transformed\n",
    "\n",
    "\n",
    "# Real-world application\n",
    "def adaptive_background_music(activity_type, intensity_level):\n",
    "    \"\"\"\n",
    "    Generate adaptive music for fitness/gaming apps.\n",
    "    \n",
    "    Args:\n",
    "        activity_type: Type of activity (running, yoga, gaming)\n",
    "        intensity_level: 0-10 intensity scale\n",
    "    \n",
    "    Returns:\n",
    "        music: Dynamically generated/adapted music\n",
    "    \"\"\"\n",
    "    # Base track selection\n",
    "    base_tracks = {\n",
    "        'running': load_track('upbeat_electronic.mid'),\n",
    "        'yoga': load_track('calm_ambient.mid'),\n",
    "        'gaming': load_track('action_soundtrack.mid')\n",
    "    }\n",
    "    \n",
    "    base = base_tracks[activity_type]\n",
    "    \n",
    "    # Adapt based on intensity\n",
    "    if intensity_level < 3:\n",
    "        # Low intensity: Calm, slow\n",
    "        music = adapt_tempo(base, target_bpm=80)\n",
    "        music = style_transfer(music, target='ambient')\n",
    "    elif intensity_level < 7:\n",
    "        # Medium intensity\n",
    "        music = adapt_tempo(base, target_bpm=120)\n",
    "    else:\n",
    "        # High intensity: Fast, energetic\n",
    "        music = adapt_tempo(base, target_bpm=160)\n",
    "        music = add_intensity(music, level=intensity_level)\n",
    "    \n",
    "    return music\n",
    "\n",
    "\n",
    "# Business applications\n",
    "def music_application_examples():\n",
    "    \"\"\"\n",
    "    Real-world music AI applications with ROI.\n",
    "    \"\"\"\n",
    "    \n",
    "    applications = {\n",
    "        'video_game_music': {\n",
    "            'description': 'Dynamic adaptive soundtracks',\n",
    "            'traditional_cost': 50000,  # Composer + recording\n",
    "            'ai_cost': 2000,\n",
    "            'roi': 96,\n",
    "            'use_case': 'Generate infinite variations based on gameplay'\n",
    "        },\n",
    "        \n",
    "        'retail_ambiance': {\n",
    "            'description': 'Custom store background music',\n",
    "            'traditional_cost': 500,  # Monthly licensing\n",
    "            'ai_cost': 50,\n",
    "            'roi': 90,\n",
    "            'use_case': 'Generate brand-specific music continuously'\n",
    "        },\n",
    "        \n",
    "        'fitness_apps': {\n",
    "            'description': 'Workout-adaptive music',\n",
    "            'traditional_cost': 10000,  # Licensed tracks\n",
    "            'ai_cost': 500,\n",
    "            'roi': 95,\n",
    "            'use_case': 'Match music tempo to heart rate/activity'\n",
    "        },\n",
    "        \n",
    "        'meditation_apps': {\n",
    "            'description': 'Personalized soundscapes',\n",
    "            'traditional_cost': 5000,\n",
    "            'ai_cost': 200,\n",
    "            'roi': 96,\n",
    "            'use_case': 'Generate calming music customized to preferences'\n",
    "        },\n",
    "        \n",
    "        'content_creation': {\n",
    "            'description': 'YouTube/podcast background music',\n",
    "            'traditional_cost': 200,  # per video licensing\n",
    "            'ai_cost': 5,\n",
    "            'roi': 97.5,\n",
    "            'use_case': 'Generate royalty-free custom music for each video'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"MUSIC AI BUSINESS APPLICATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for app_name, details in applications.items():\n",
    "        savings = details['traditional_cost'] - details['ai_cost']\n",
    "        roi = details['roi']\n",
    "        \n",
    "        print(f\"\\n{app_name.upper().replace('_', ' ')}\")\n",
    "        print(f\"  Use Case: {details['use_case']}\")\n",
    "        print(f\"  Traditional Cost: ${details['traditional_cost']:,}\")\n",
    "        print(f\"  AI Cost: ${details['ai_cost']:,}\")\n",
    "        print(f\"  Savings: ${savings:,} ({roi}%)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    music_application_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecba83",
   "metadata": {},
   "source": [
    "**Music AI Market:**\n",
    "- **Size:** $1.5B in 2023 → $5B by 2028\n",
    "- **Growth:** 30% CAGR\n",
    "- **Key Players:** Spotify, YouTube, TikTok, gaming companies\n",
    "\n",
    "**Ethical Considerations:**\n",
    "- Copyright and ownership\n",
    "- Musician displacement concerns\n",
    "- Authenticity vs AI-generated\n",
    "- Fair compensation models\n",
    "\n",
    "**Best Practices:**\n",
    "- Transparent AI usage disclosure\n",
    "- Hybrid human-AI workflows\n",
    "- Support for human musicians\n",
    "- Responsible licensing\n",
    "\n",
    "---\n",
    "\n",
    "**End of Batch 4 (Slides 21-25)**\n",
    "\n",
    "*Continue to Batch 5 for Advanced Topics (Slides 26-30)*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
