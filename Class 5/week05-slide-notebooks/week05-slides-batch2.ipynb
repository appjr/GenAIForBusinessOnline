{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a1c4a8",
   "metadata": {},
   "source": [
    "# Week05 Slides Batch2\n",
    "\n",
    "**Interactive Jupyter Notebook Version**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6d0e64",
   "metadata": {},
   "source": [
    "# Week 5: Image Generation, Audio, and Music - Slides Batch 2 (Slides 11-15)\n",
    "\n",
    "**Course:** BUAN 6v99.SW2 - Generative AI for Business  \n",
    "**Continuation from Batch 1**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9748426",
   "metadata": {},
   "source": [
    "Diffusion Models - The Math\n",
    "\n",
    "### Understanding the Diffusion Process\n",
    "\n",
    "**Forward Diffusion (Adding Noise):**\n",
    "\n",
    "At each timestep t, we add a small amount of Gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "q(x_t | x_{t-1}) = N(x_t; √(1-β_t) * x_{t-1}, β_t * I)\n",
    "\n",
    "Where:\n",
    "- x_t: Image at timestep t\n",
    "- β_t: Noise schedule (how much noise to add)\n",
    "- Starts with clean image (t=0)\n",
    "- Ends with pure noise (t=T, typically T=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f28546b",
   "metadata": {},
   "source": [
    "**Key Property:** After enough steps, image becomes pure Gaussian noise\n",
    "\n",
    "**Reverse Diffusion (Denoising):**\n",
    "\n",
    "Learn to reverse the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aac641",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_θ(x_{t-1} | x_t) = N(x_{t-1}; μ_θ(x_t, t), Σ_θ(x_t, t))\n",
    "\n",
    "Where:\n",
    "- μ_θ: Predicted mean (what neural network learns)\n",
    "- Σ_θ: Predicted variance\n",
    "- θ: Neural network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4541727",
   "metadata": {},
   "source": [
    "**Training Objective:**\n",
    "\n",
    "Learn to predict the noise that was added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5092c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = E_{t, x_0, ε} [||ε - ε_θ(x_t, t)||²]\n",
    "\n",
    "Where:\n",
    "- ε: True noise added\n",
    "- ε_θ: Predicted noise from network\n",
    "- Simply predict what noise was added!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a1ce6",
   "metadata": {},
   "source": [
    "**Why This Works:**\n",
    "\n",
    "1. **Simple objective:** Just predict noise\n",
    "2. **Gradual refinement:** 1000 small steps vs 1 big step\n",
    "3. **Stable training:** No adversarial dynamics like GANs\n",
    "4. **High quality:** Time to refine details\n",
    "\n",
    "**Intuitive Analogy:**\n",
    "\n",
    "Imagine sculpting:\n",
    "- Start with rough block (noisy image)\n",
    "- Gradually refine with small touches (denoising steps)\n",
    "- Each step reveals more detail\n",
    "- Final result: Detailed sculpture (clean image)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a7d49a",
   "metadata": {},
   "source": [
    "Implementing a Simple Diffusion Model\n",
    "\n",
    "### DDPM (Denoising Diffusion Probabilistic Models)\n",
    "\n",
    "**Complete Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e192aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SimpleDiffusion:\n",
    "    \"\"\"\n",
    "    Simple Diffusion Model for image generation.\n",
    "    \n",
    "    Implements DDPM (Denoising Diffusion Probabilistic Models).\n",
    "    \n",
    "    Reference: \"Denoising Diffusion Probabilistic Models\" (Ho et al., 2020)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, timesteps=1000, beta_start=0.0001, beta_end=0.02, device='cpu'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            timesteps: Number of diffusion steps (T)\n",
    "            beta_start: Initial noise level\n",
    "            beta_end: Final noise level\n",
    "            device: 'cpu' or 'cuda'\n",
    "        \"\"\"\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Linear noise schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n",
    "        \n",
    "        # Pre-compute useful quantities\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Calculations for diffusion q(x_t | x_0)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        \n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = (\n",
    "            self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "        )\n",
    "    \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: Add noise to image.\n",
    "        \n",
    "        q(x_t | x_0) = N(x_t; √α̅_t * x_0, (1 - α̅_t) * I)\n",
    "        \n",
    "        Args:\n",
    "            x_start: Original image\n",
    "            t: Timestep\n",
    "            noise: Optional pre-generated noise\n",
    "        \n",
    "        Returns:\n",
    "            Noisy image at timestep t\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].reshape(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1, 1, 1)\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    def p_losses(self, denoise_model, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Training loss: MSE between true noise and predicted noise.\n",
    "        \n",
    "        Args:\n",
    "            denoise_model: Neural network that predicts noise\n",
    "            x_start: Original image\n",
    "            t: Timestep\n",
    "            noise: Optional pre-generated noise\n",
    "        \n",
    "        Returns:\n",
    "            Loss value\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        \n",
    "        # Add noise to image\n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = denoise_model(x_noisy, t)\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, denoise_model, x, t):\n",
    "        \"\"\"\n",
    "        Reverse diffusion: Remove noise from image (single step).\n",
    "        \n",
    "        Args:\n",
    "            denoise_model: Trained denoising model\n",
    "            x: Noisy image at timestep t\n",
    "            t: Current timestep\n",
    "        \n",
    "        Returns:\n",
    "            Less noisy image at timestep t-1\n",
    "        \"\"\"\n",
    "        # Predict noise\n",
    "        predicted_noise = denoise_model(x, t)\n",
    "        \n",
    "        # Calculate x_{t-1}\n",
    "        betas_t = self.betas[t].reshape(-1, 1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].reshape(-1, 1, 1, 1)\n",
    "        sqrt_recip_alphas_t = torch.sqrt(1.0 / self.alphas[t]).reshape(-1, 1, 1, 1)\n",
    "        \n",
    "        # Mean of p(x_{t-1} | x_t)\n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x - betas_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "        \n",
    "        if t[0] == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = self.posterior_variance[t].reshape(-1, 1, 1, 1)\n",
    "            noise = torch.randn_like(x)\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, denoise_model, shape):\n",
    "        \"\"\"\n",
    "        Generate image by iteratively denoising.\n",
    "        \n",
    "        Start from pure noise, denoise for T steps.\n",
    "        \n",
    "        Args:\n",
    "            denoise_model: Trained denoising model\n",
    "            shape: Shape of image to generate (batch, channels, height, width)\n",
    "        \n",
    "        Returns:\n",
    "            Generated images\n",
    "        \"\"\"\n",
    "        device = next(denoise_model.parameters()).device\n",
    "        b = shape[0]\n",
    "        \n",
    "        # Start from pure noise\n",
    "        img = torch.randn(shape, device=device)\n",
    "        \n",
    "        # Progressively denoise\n",
    "        for i in reversed(range(0, self.timesteps)):\n",
    "            t = torch.full((b,), i, device=device, dtype=torch.long)\n",
    "            img = self.p_sample(denoise_model, img, t)\n",
    "        \n",
    "        return img\n",
    "\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified U-Net for denoising.\n",
    "    \n",
    "    Takes noisy image and timestep, predicts noise.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels=1, time_emb_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(1, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "        \n",
    "        # Downsampling\n",
    "        self.conv1 = nn.Conv2d(channels, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        \n",
    "        # Upsampling\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.conv4 = nn.Conv2d(256 + 128, 128, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(128 + 64, 64, 3, padding=1)\n",
    "        \n",
    "        # Output\n",
    "        self.conv_out = nn.Conv2d(64, channels, 3, padding=1)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        # Time embedding\n",
    "        t = t.float().unsqueeze(-1) / 1000.0\n",
    "        t_emb = self.time_mlp(t)\n",
    "        t_emb = t_emb.unsqueeze(-1).unsqueeze(-1)\n",
    "        \n",
    "        # Encoder\n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        x2 = self.pool(x1)\n",
    "        x2 = F.relu(self.conv2(x2))\n",
    "        x3 = self.pool(x2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x3 = F.relu(self.conv3(x3))\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up(x3)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        x = self.up(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        \n",
    "        return self.conv_out(x)\n",
    "\n",
    "\n",
    "def train_diffusion(model, diffusion, dataloader, epochs, device):\n",
    "    \"\"\"Train diffusion model\"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"TRAINING DIFFUSION MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (images, _) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            batch_size = images.shape[0]\n",
    "            \n",
    "            # Sample random timesteps\n",
    "            t = torch.randint(0, diffusion.timesteps, (batch_size,), device=device).long()\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = diffusion.p_losses(model, images, t)\n",
    "            \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f\"Epoch {epoch} [{batch_idx}/{len(dataloader)}] Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch} Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Generate samples every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                samples = diffusion.p_sample_loop(model, shape=(16, 1, 28, 28))\n",
    "                samples = samples.cpu()\n",
    "                \n",
    "                fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
    "                for i, ax in enumerate(axes.flat):\n",
    "                    ax.imshow(samples[i].squeeze(), cmap='gray')\n",
    "                    ax.axis('off')\n",
    "                plt.suptitle(f'Generated Images - Epoch {epoch}')\n",
    "                plt.savefig(f'diffusion_epoch_{epoch}.png', dpi=150)\n",
    "                plt.close()\n",
    "            model.train()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*70)\n",
    "    print(\"DIFFUSION MODEL TRAINING ON MNIST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    \n",
    "    # Load MNIST\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    \n",
    "    # Create model and diffusion\n",
    "    model = SimpleUNet().to(device)\n",
    "    diffusion = SimpleDiffusion(timesteps=1000, device=device)\n",
    "    \n",
    "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"Diffusion timesteps: {diffusion.timesteps}\")\n",
    "    \n",
    "    # Train\n",
    "    model = train_diffusion(model, diffusion, dataloader, epochs=10, device=device)\n",
    "    \n",
    "    # Generate new images\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING NEW IMAGES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        samples = diffusion.p_sample_loop(model, shape=(64, 1, 28, 28))\n",
    "        samples = samples.cpu()\n",
    "        \n",
    "        fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            ax.imshow(samples[i].squeeze(), cmap='gray')\n",
    "            ax.axis('off')\n",
    "        plt.suptitle('Final Generated Images from Diffusion Model', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('diffusion_final_samples.png', dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Training complete! Model can generate images from pure noise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc878a3",
   "metadata": {},
   "source": [
    "**Key Points:**\n",
    "\n",
    "1. **Forward process:** Gradually add noise (deterministic)\n",
    "2. **Reverse process:** Learn to remove noise (neural network)\n",
    "3. **Training:** Predict the noise that was added\n",
    "4. **Generation:** Start with noise, denoise iteratively\n",
    "\n",
    "**Advantages over GANs:**\n",
    "- ✅ Stable training (no adversarial dynamics)\n",
    "- ✅ Better quality outputs\n",
    "- ✅ No mode collapse\n",
    "- ✅ Meaningful training curves\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe558e",
   "metadata": {},
   "source": [
    "Text-to-Image with Stable Diffusion\n",
    "\n",
    "### Adding Text Conditioning\n",
    "\n",
    "**The Revolution:** Combine diffusion with text embeddings!\n",
    "\n",
    "**Architecture:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada23c2",
   "metadata": {},
   "source": [
    "```python\n",
    "Text Prompt → Text Encoder (CLIP) → Text Embeddings\n",
    "                                           ↓\n",
    "Pure Noise → U-Net (conditioned on text) → Denoised Image\n",
    "                    ↑                          ↓\n",
    "                Timestep                   Repeat 50x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad423a",
   "metadata": {},
   "source": [
    "**Key Components:**\n",
    "\n",
    "**1. Text Encoder (CLIP)**\n",
    "- Converts text to embeddings\n",
    "- Pre-trained on 400M image-text pairs\n",
    "- Captures semantic meaning\n",
    "\n",
    "**2. Latent Diffusion**\n",
    "- Work in compressed latent space (8x8x4 instead of 512x512x3)\n",
    "- 64x faster than pixel-space diffusion\n",
    "- Same quality, much more efficient\n",
    "\n",
    "**3. Cross-Attention**\n",
    "- U-Net attends to text embeddings\n",
    "- Guides generation based on prompt\n",
    "- Different layers attend to different concepts\n",
    "\n",
    "**Stable Diffusion Pipeline:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "def generate_image(prompt, negative_prompt=\"\", num_steps=50):\n",
    "    \"\"\"\n",
    "    Generate image from text using Stable Diffusion.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Text description\n",
    "        negative_prompt: What to avoid\n",
    "        num_steps: Number of denoising steps\n",
    "    \n",
    "    Returns:\n",
    "        Generated PIL image\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    pipe = pipe.to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=num_steps,\n",
    "        guidance_scale=7.5  # How strictly to follow prompt\n",
    "    ).images[0]\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "# Example usage\n",
    "prompt = \"A beautiful sunset over mountains, oil painting style, highly detailed\"\n",
    "negative_prompt = \"blurry, low quality, distorted\"\n",
    "\n",
    "image = generate_image(prompt, negative_prompt)\n",
    "image.save(\"generated_sunset.png\")\n",
    "print(\"✓ Image generated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c044445",
   "metadata": {},
   "source": [
    "**Prompt Engineering Tips:**\n",
    "\n",
    "**Good Prompts:**\n",
    "- \"A photorealistic portrait of a cat wearing a top hat, studio lighting, 4k\"\n",
    "- \"Cyberpunk city at night, neon lights, raining, cinematic, highly detailed\"\n",
    "- \"Watercolor painting of a peaceful garden, soft colors, artistic\"\n",
    "\n",
    "**Bad Prompts:**\n",
    "- \"cat\" (too vague)\n",
    "- \"nice picture\" (not descriptive)\n",
    "- \"something cool\" (no specifics)\n",
    "\n",
    "**Modifiers that work:**\n",
    "- Style: photorealistic, oil painting, watercolor, digital art\n",
    "- Quality: highly detailed, 4k, masterpiece, trending on artstation\n",
    "- Lighting: studio lighting, golden hour, dramatic lighting\n",
    "- Camera: wide angle, close-up, aerial view\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a3ed4c",
   "metadata": {},
   "source": [
    "Advanced Diffusion Techniques\n",
    "\n",
    "### Making Diffusion Faster and Better\n",
    "\n",
    "**Problem:** Standard diffusion requires ~1000 steps (slow!)\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "**1. DDIM (Denoising Diffusion Implicit Models)**\n",
    "- Deterministic sampling (same noise → same image)\n",
    "- Fewer steps: 50 instead of 1000 (20x faster!)\n",
    "- Nearly same quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b8743",
   "metadata": {},
   "source": [
    "**2. Latent Diffusion (Stable Diffusion)**\n",
    "- Compress image to latent space first (using VAE)\n",
    "- Run diffusion in latent space (much smaller)\n",
    "- Decode back to image\n",
    "- **64x faster** than pixel-space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be4845",
   "metadata": {},
   "source": [
    "```python\n",
    "High-res Image (512x512x3) → VAE Encode → Latent (64x64x4)\n",
    "                                                ↓\n",
    "                                          Diffusion here!\n",
    "                                                ↓\n",
    "                                 VAE Decode ← Denoised Latent\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f2271c",
   "metadata": {},
   "source": [
    "**3. Classifier-Free Guidance**\n",
    "- Stronger control over text conditioning\n",
    "- Mix conditional and unconditional predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7957d1",
   "metadata": {},
   "source": [
    "**Speed Comparisons:**\n",
    "\n",
    "| Method | Steps | Time | Quality |\n",
    "|--------|-------|------|---------|\n",
    "| DDPM | 1000 | 60s | ⭐⭐⭐⭐⭐ |\n",
    "| DDIM | 50 | 3s | ⭐⭐⭐⭐⭐ |\n",
    "| Latent Diffusion | 50 | 2s | ⭐⭐⭐⭐⭐ |\n",
    "| Latent + Distillation | 4 | 0.5s | ⭐⭐⭐⭐ |\n",
    "\n",
    "**Business Impact:**\n",
    "- Faster = More images per dollar\n",
    "- Real-time applications become possible\n",
    "- Reduced infrastructure costs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2430a2b5",
   "metadata": {},
   "source": [
    "Image Editing with Diffusion\n",
    "\n",
    "### Beyond Generation: Editing Existing Images\n",
    "\n",
    "**Powerful Applications:**\n",
    "\n",
    "**1. Inpainting (Fill in Missing Parts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d43b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "\n",
    "def inpaint_image(image, mask, prompt):\n",
    "    \"\"\"\n",
    "    Fill in masked region based on prompt.\n",
    "    \n",
    "    Args:\n",
    "        image: Original PIL image\n",
    "        mask: Binary mask (white = fill in)\n",
    "        prompt: What to generate in masked area\n",
    "    \n",
    "    Returns:\n",
    "        Edited image\n",
    "    \"\"\"\n",
    "    pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-inpainting\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    result = pipe(\n",
    "        prompt=prompt,\n",
    "        image=image,\n",
    "        mask_image=mask\n",
    "    ).images[0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example: Remove person from photo\n",
    "prompt = \"empty park bench, natural lighting\"\n",
    "edited = inpaint_image(photo, person_mask, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e7ddd",
   "metadata": {},
   "source": [
    "**Use Cases:**\n",
    "- Remove unwanted objects\n",
    "- Fill in missing parts\n",
    "- Extend images (outpainting)\n",
    "- Product placement\n",
    "\n",
    "**2. Image-to-Image Translation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e952f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "\n",
    "def style_transfer(image, prompt, strength=0.75):\n",
    "    \"\"\"\n",
    "    Transform image based on prompt.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image\n",
    "        prompt: Target style/content\n",
    "        strength: How much to change (0=no change, 1=complete change)\n",
    "    \n",
    "    Returns:\n",
    "        Styled image\n",
    "    \"\"\"\n",
    "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    result = pipe(\n",
    "        prompt=prompt,\n",
    "        image=image,\n",
    "        strength=strength,\n",
    "        guidance_scale=7.5\n",
    "    ).images[0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example: Make photo look like oil painting\n",
    "prompt = \"oil painting, impressionist style, vibrant colors\"\n",
    "artistic = style_transfer(photo, prompt, strength=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164241d3",
   "metadata": {},
   "source": [
    "**Use Cases:**\n",
    "- Artistic style transfer\n",
    "- Photo enhancement\n",
    "- Sketch to photo\n",
    "- Season transfer (summer → winter)\n",
    "\n",
    "**3. ControlNet (Precise Control)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f05f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
    "from controlnet_aux import HEDdetector\n",
    "\n",
    "def controlled_generation(image, prompt, control_type=\"canny\"):\n",
    "    \"\"\"\n",
    "    Generate image with structure preserved.\n",
    "    \n",
    "    Args:\n",
    "        image: Control image (edges, pose, depth, etc.)\n",
    "        prompt: What to generate\n",
    "        control_type: Type of control signal\n",
    "    \n",
    "    Returns:\n",
    "        Generated image following structure\n",
    "    \"\"\"\n",
    "    # Load ControlNet\n",
    "    controlnet = ControlNetModel.from_pretrained(\n",
    "        f\"lllyasviel/sd-controlnet-{control_type}\"\n",
    "    )\n",
    "    \n",
    "    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        controlnet=controlnet\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate\n",
    "    result = pipe(\n",
    "        prompt=prompt,\n",
    "        image=image,\n",
    "        num_inference_steps=50\n",
    "    ).images[0]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example: Generate image matching sketch edges\n",
    "prompt = \"beautiful landscape, photorealistic, 4k\"\n",
    "generated = controlled_generation(sketch_edges, prompt, control_type=\"canny\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6908ebf4",
   "metadata": {},
   "source": [
    "**ControlNet Types:**\n",
    "- **Canny edges:** Preserve edges/structure\n",
    "- **Depth map:** Preserve 3D geometry\n",
    "- **Pose:** Preserve human poses\n",
    "- **Segmentation:** Preserve object layout\n",
    "\n",
    "**Business Applications:**\n",
    "\n",
    "**Architecture/Interior Design:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3861b3",
   "metadata": {},
   "source": [
    "**Fashion:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66779c7",
   "metadata": {},
   "source": [
    "**Marketing:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f76dc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**End of Batch 2 (Slides 11-15)**\n",
    "\n",
    "*Continue to Batch 3 for Audio & Music Generation (Slides 16-20)*"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
