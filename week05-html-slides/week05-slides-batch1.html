<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>week05-slides-batch1</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        h3 {
            color: #7f8c8d;
        }
        pre {
            background-color: #2c3e50;
            color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            font-family: 'Courier New', monospace;
            font-size: 14px;
        }
        li {
            margin: 5px 0;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #3498db;
            color: white;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Week 5: Image Generation, Audio, and Music - Slides Batch 1 (Slides 1-10)</h1>

<strong>Course:</strong> BUAN 6v99.SW2 - Generative AI for Business  
<strong>Date:</strong> February 17, 2026  
<strong>Duration:</strong> 2.5 hours

---

<h2>Slide 1: Week 5 Title Slide</h2>

<h3>Image Generation, Audio, and Music with GenAI</h3>

<strong>Today's Focus:</strong>
<li>Understanding generative models beyond text</li>
<li>VAEs, GANs, and Diffusion Models</li>
<li>Creating images from text prompts</li>
<li>Audio and music generation</li>
<li>Multimodal AI applications</li>
<li>Business use cases in creative industries</li>

<strong>Prerequisites:</strong>
<li>Week 4: Deep Learning and Transformers</li>
<li>Understanding of neural networks</li>
<li>Basic PyTorch/TensorFlow knowledge</li>

---

<h2>Slide 2: Today's Agenda</h2>

<h3>Class Overview</h3>

1. <strong>Image Generation Fundamentals</strong> (30 min)
2. <strong>VAEs and GANs</strong> (30 min)
3. <strong>Break</strong> (10 min)
4. <strong>Diffusion Models</strong> (35 min)
5. <strong>Audio and Music Generation</strong> (25 min)
6. <strong>Business Applications</strong> (20 min)
7. <strong>Hands-on Lab & Q&A</strong> (20 min)

---

<h2>Slide 3: Learning Objectives</h2>

<h3>By the End of This Class, You Will:</h3>

‚úÖ <strong>Understand</strong> how generative models create images and audio  
‚úÖ <strong>Explain</strong> VAEs, GANs, and Diffusion Models  
‚úÖ <strong>Implement</strong> basic image generation systems  
‚úÖ <strong>Recognize</strong> audio generation techniques  
‚úÖ <strong>Apply</strong> these technologies to business problems  
‚úÖ <strong>Evaluate</strong> ROI of creative GenAI applications

---

<h2>Slide 4: The Evolution of Image Generation</h2>

<h3>From Rules to Neural Networks</h3>

<strong>Historical Timeline:</strong>

<strong>1. Rule-Based Graphics (1960s-1990s)</strong>
<li>Computer graphics with manual programming</li>
<li>3D rendering engines</li>
<li>Procedural generation</li>
<li>Limited creativity, deterministic</li>

<strong>2. Style Transfer (2015)</strong>
<li>Neural Style Transfer using CNNs</li>
<li>Combine content of one image with style of another</li>
<li>First neural network "art"</li>
<li>Example: Photo in Van Gogh's style</li>

<strong>3. GANs (2014-2020)</strong>
<li>Generate realistic faces, objects, scenes</li>
<li>Progressive improvements (StyleGAN)</li>
<li>High-resolution synthesis</li>
<li>Limited control over output</li>

<strong>4. Diffusion Models (2020-Present)</strong>
<li>DALL-E, Stable Diffusion, Midjourney</li>
<li>Text-to-image generation</li>
<li>Precise control via prompts</li>
<li>Revolutionary creative tool</li>

<strong>Business Impact:</strong>
<li>Design automation</li>
<li>Content creation at scale</li>
<li>Personalization</li>
<li>Cost reduction: $50-200 per image ‚Üí $0.01</li>

---

<h2>Slide 5: Variational Autoencoders (VAEs)</h2>

<h3>Learning Compressed Representations</h3>

<strong>What is a VAE?</strong>

A Variational Autoencoder learns to:
1. <strong>Encode</strong> images into a compressed latent space
2. <strong>Sample</strong> from that latent space
3. <strong>Decode</strong> samples back into images

<strong>Architecture:</strong>
<pre><code>Image ‚Üí Encoder ‚Üí Latent Space (Œº, œÉ) ‚Üí Decoder ‚Üí Reconstructed Image
                      ‚Üì
                   Sample z ~ N(Œº, œÉ)
                      ‚Üì
                   Decoder ‚Üí New Image
</code></pre>

<strong>Key Insight:</strong> The latent space is <strong>continuous and smooth</strong>, meaning:
<li>Similar images cluster together</li>
<li>Interpolation between images is meaningful</li>
<li>We can sample to generate new images</li>

<strong>Complete VAE Implementation:</strong>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

class VAE(nn.Module):
    """
    Variational Autoencoder for image generation.
    
    Architecture:
        Encoder: Image ‚Üí Latent distribution (Œº, log_var)
        Decoder: Latent sample ‚Üí Reconstructed image
    
    Loss:
        Reconstruction loss + KL divergence
    """
    
    def __init__(self, latent_dim=20, input_channels=1, hidden_dim=400):
        """
        Args:
            latent_dim: Dimension of latent space
            input_channels: 1 for grayscale, 3 for RGB
            hidden_dim: Hidden layer dimension
        """
        super(VAE, self).__init__()
        
        self.latent_dim = latent_dim
        
        # Encoder: Image ‚Üí Hidden ‚Üí Latent parameters
        self.fc1 = nn.Linear(input_channels <em> 28 </em> 28, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        
        # Decoder: Latent ‚Üí Hidden ‚Üí Image
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_channels <em> 28 </em> 28)
    
    def encode(self, x):
        """
        Encode image to latent distribution parameters.
        
        Args:
            x: Input images (batch, channels, height, width)
        
        Returns:
            mu: Mean of latent distribution
            logvar: Log variance of latent distribution
        """
        # Flatten image
        x = x.view(-1, 28 * 28)
        
        # Encoder forward pass
        h = F.relu(self.fc1(x))
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        """
        Reparameterization trick: z = Œº + œÉ * Œµ, where Œµ ~ N(0,1)
        
        This allows backpropagation through sampling.
        
        Args:
            mu: Mean
            logvar: Log variance
        
        Returns:
            z: Sampled latent vector
        """
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        z = mu + eps * std
        return z
    
    def decode(self, z):
        """
        Decode latent vector to image.
        
        Args:
            z: Latent vector
        
        Returns:
            Reconstructed image
        """
        h = F.relu(self.fc3(z))
        x_reconstructed = torch.sigmoid(self.fc4(h))
        return x_reconstructed.view(-1, 1, 28, 28)
    
    def forward(self, x):
        """Full forward pass: encode, sample, decode"""
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_reconstructed = self.decode(z)
        return x_reconstructed, mu, logvar


def vae_loss(x_reconstructed, x, mu, logvar):
    """
    VAE loss = Reconstruction loss + KL divergence
    
    Reconstruction loss: How well we reconstruct input
    KL divergence: How close latent distribution is to N(0,1)
    
    Args:
        x_reconstructed: Reconstructed images
        x: Original images
        mu: Mean of latent distribution
        logvar: Log variance of latent distribution
    
    Returns:
        Total loss
    """
    # Reconstruction loss (binary cross-entropy)
    BCE = F.binary_cross_entropy(
        x_reconstructed.view(-1, 28*28), 
        x.view(-1, 28*28), 
        reduction='sum'
    )
    
    # KL divergence: KL(N(Œº,œÉ¬≤) || N(0,1))
    # = -0.5 * sum(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    return BCE + KLD


def train_vae(model, train_loader, optimizer, epoch, device):
    """Train VAE for one epoch"""
    model.train()
    train_loss = 0
    
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.to(device)
        optimizer.zero_grad()
        
        # Forward pass
        x_reconstructed, mu, logvar = model(data)
        loss = vae_loss(x_reconstructed, data, mu, logvar)
        
        # Backward pass
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
        
        if batch_idx % 100 == 0:
            print(f'Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] '
                  f'Loss: {loss.item() / len(data):.4f}')
    
    avg_loss = train_loss / len(train_loader.dataset)
    print(f'====> Epoch: {epoch} Average loss: {avg_loss:.4f}')
    return avg_loss


<h1>Complete training example</h1>
if __name__ == "__main__":
    print("="*70)
    print("VARIATIONAL AUTOENCODER (VAE) TRAINING")
    print("="*70)
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nUsing device: {device}")
    
    # Load MNIST
    print("\nLoading MNIST dataset...")
    transform = transforms.Compose([
        transforms.ToTensor(),
    ])
    
    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
    
    # Create model
    latent_dim = 20
    model = VAE(latent_dim=latent_dim).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    
    print(f"\nModel Configuration:")
    print(f"  Latent dimension: {latent_dim}")
    print(f"  Parameters: {sum(p.numel() for p in model.parameters()):,}")
    
    # Train
    print("\n" + "="*70)
    print("TRAINING")
    print("="*70)
    
    n_epochs = 10
    losses = []
    
    for epoch in range(1, n_epochs + 1):
        loss = train_vae(model, train_loader, optimizer, epoch, device)
        losses.append(loss)
    
    # Generate new images
    print("\n" + "="*70)
    print("GENERATING NEW IMAGES")
    print("="*70)
    
    model.eval()
    with torch.no_grad():
        # Sample from standard normal
        z = torch.randn(64, latent_dim).to(device)
        samples = model.decode(z).cpu()
        
        # Visualize
        fig, axes = plt.subplots(8, 8, figsize=(10, 10))
        for i, ax in enumerate(axes.flat):
            ax.imshow(samples[i].squeeze(), cmap='gray')
            ax.axis('off')
        
        plt.suptitle('Generated Images from VAE', fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig('vae_generated_images.png', dpi=300, bbox_inches='tight')
        print("\n‚úì Saved generated images to 'vae_generated_images.png'")
        plt.show()
    
    # Visualize latent space
    print("\nInterpolating in latent space...")
    with torch.no_grad():
        # Two random points
        z1 = torch.randn(1, latent_dim).to(device)
        z2 = torch.randn(1, latent_dim).to(device)
        
        # Interpolate
        fig, axes = plt.subplots(1, 10, figsize=(20, 2))
        for i, alpha in enumerate(np.linspace(0, 1, 10)):
            z = (1 - alpha) <em> z1 + alpha </em> z2
            img = model.decode(z).cpu()
            axes[i].imshow(img.squeeze(), cmap='gray')
            axes[i].axis('off')
            axes[i].set_title(f'{alpha:.1f}')
        
        plt.suptitle('Latent Space Interpolation', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig('vae_interpolation.png', dpi=300, bbox_inches='tight')
        print("‚úì Saved interpolation to 'vae_interpolation.png'")
        plt.show()
    
    print("\n" + "="*70)
    print("TRAINING COMPLETE!")
    print("="*70)
    print(f"Final loss: {losses[-1]:.4f}")
    print("Model can now generate new images by sampling from latent space!")
</code></pre>

<strong>Key Advantages:</strong>
<li>‚úÖ Smooth latent space (good for interpolation)</li>
<li>‚úÖ Probabilistic framework (quantifies uncertainty)</li>
<li>‚úÖ Easy to train (stable)</li>

<strong>Limitations:</strong>
<li>‚ùå Blurry outputs (due to reconstruction loss)</li>
<li>‚ùå Less realistic than GANs</li>
<li>‚ùå Limited diversity</li>

<strong>Business Applications:</strong>
<li>Product design variations</li>
<li>Data augmentation</li>
<li>Anomaly detection</li>
<li>Dimensionality reduction</li>

---

<h2>Slide 6: Generative Adversarial Networks (GANs)</h2>

<h3>A Game Between Generator and Discriminator</h3>

<strong>What is a GAN?</strong>

Two neural networks compete in a game:

<strong>Generator (G):</strong>
<li>Creates fake images from random noise</li>
<li>Goal: Fool the discriminator</li>
<li>"Counterfeiter making fake money"</li>

<strong>Discriminator (D):</strong>
<li>Distinguishes real from fake images</li>
<li>Goal: Detect generator's fakes</li>
<li>"Police detecting counterfeit money"</li>

<strong>Training Process:</strong>
<pre><code>1. Generator creates fake images
2. Discriminator sees real + fake images
3. Discriminator learns to tell them apart
4. Generator learns to fool discriminator better
5. Repeat until generator produces realistic images
</code></pre>

<strong>Mathematical Framework:</strong>
<pre><code>min_G max_D V(D,G) = E_x[log D(x)] + E_z[log(1 - D(G(z)))]

Where:
<li>D(x): Discriminator's probability that x is real</li>
<li>G(z): Generator's output from noise z</li>
<li>Real images maximize D(x)</li>
<li>Fake images minimize D(G(z))</li>
</code></pre>

<strong>Complete GAN Implementation:</strong>

<pre><code class="language-python">import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt
import numpy as np

class Generator(nn.Module):
    """
    Generator network: Noise ‚Üí Image
    
    Takes random noise and generates realistic images.
    Uses transposed convolutions to upsample.
    """
    
    def __init__(self, latent_dim=100, img_channels=1, feature_dim=64):
        """
        Args:
            latent_dim: Dimension of input noise vector
            img_channels: Number of image channels (1 for grayscale, 3 for RGB)
            feature_dim: Base feature map size
        """
        super(Generator, self).__init__()
        
        self.latent_dim = latent_dim
        
        # Network: Noise (100) ‚Üí 7x7x256 ‚Üí 14x14x128 ‚Üí 28x28x1
        self.model = nn.Sequential(
            # Input: (batch, latent_dim, 1, 1)
            nn.ConvTranspose2d(latent_dim, feature_dim * 4, 7, 1, 0, bias=False),
            nn.BatchNorm2d(feature_dim * 4),
            nn.ReLU(True),
            # State: (batch, feature_dim*4, 7, 7)
            
            nn.ConvTranspose2d(feature_dim <em> 4, feature_dim </em> 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 2),
            nn.ReLU(True),
            # State: (batch, feature_dim*2, 14, 14)
            
            nn.ConvTranspose2d(feature_dim * 2, img_channels, 4, 2, 1, bias=False),
            nn.Tanh()
            # Output: (batch, img_channels, 28, 28), values in [-1, 1]
        )
    
    def forward(self, z):
        """Generate image from noise"""
        img = self.model(z)
        return img


class Discriminator(nn.Module):
    """
    Discriminator network: Image ‚Üí Real/Fake probability
    
    Classifies images as real or fake.
    Uses standard convolutions to downsample.
    """
    
    def __init__(self, img_channels=1, feature_dim=64):
        """
        Args:
            img_channels: Number of image channels
            feature_dim: Base feature map size
        """
        super(Discriminator, self).__init__()
        
        # Network: 28x28x1 ‚Üí 14x14x64 ‚Üí 7x7x128 ‚Üí 1
        self.model = nn.Sequential(
            # Input: (batch, img_channels, 28, 28)
            nn.Conv2d(img_channels, feature_dim, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (batch, feature_dim, 14, 14)
            
            nn.Conv2d(feature_dim, feature_dim * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(feature_dim * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # State: (batch, feature_dim*2, 7, 7)
            
            nn.Conv2d(feature_dim * 2, 1, 7, 1, 0, bias=False),
            nn.Sigmoid()
            # Output: (batch, 1, 1, 1) ‚Üí probability
        )
    
    def forward(self, img):
        """Classify image as real or fake"""
        validity = self.model(img)
        return validity.view(-1, 1)


def train_gan(generator, discriminator, dataloader, n_epochs, latent_dim, device):
    """
    Train GAN using alternating optimization.
    
    Training loop:
        1. Train Discriminator on real + fake images
        2. Train Generator to fool Discriminator
    """
    
    # Loss function: Binary Cross-Entropy
    criterion = nn.BCELoss()
    
    # Optimizers
    optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    
    # Training history
    d_losses = []
    g_losses = []
    
    print("\n" + "="*70)
    print("GAN TRAINING")
    print("="*70)
    
    for epoch in range(n_epochs):
        for i, (real_imgs, _) in enumerate(dataloader):
            batch_size = real_imgs.size(0)
            real_imgs = real_imgs.to(device)
            
            # Labels for real and fake
            real_labels = torch.ones(batch_size, 1).to(device)
            fake_labels = torch.zeros(batch_size, 1).to(device)
            
            # ---------------------
            #  Train Discriminator
            # ---------------------
            optimizer_D.zero_grad()
            
            # Real images
            real_validity = discriminator(real_imgs)
            d_real_loss = criterion(real_validity, real_labels)
            
            # Fake images
            z = torch.randn(batch_size, latent_dim, 1, 1).to(device)
            fake_imgs = generator(z)
            fake_validity = discriminator(fake_imgs.detach())
            d_fake_loss = criterion(fake_validity, fake_labels)
            
            # Total discriminator loss
            d_loss = (d_real_loss + d_fake_loss) / 2
            d_loss.backward()
            optimizer_D.step()
            
            # -----------------
            #  Train Generator
            # -----------------
            optimizer_G.zero_grad()
            
            # Generate images and try to fool discriminator
            fake_validity = discriminator(fake_imgs)
            g_loss = criterion(fake_validity, real_labels)  # Want discriminator to think they're real
            
            g_loss.backward()
            optimizer_G.step()
            
            # Logging
            if i % 100 == 0:
                print(f"Epoch [{epoch}/{n_epochs}] Batch [{i}/{len(dataloader)}] "
                      f"D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}")
        
        d_losses.append(d_loss.item())
        g_losses.append(g_loss.item())
        
        # Generate samples every epoch
        if epoch % 5 == 0:
            with torch.no_grad():
                z = torch.randn(16, latent_dim, 1, 1).to(device)
                gen_imgs = generator(z).cpu()
                
                fig, axes = plt.subplots(4, 4, figsize=(8, 8))
                for idx, ax in enumerate(axes.flat):
                    ax.imshow(gen_imgs[idx].squeeze(), cmap='gray')
                    ax.axis('off')
                plt.suptitle(f'Generated Images - Epoch {epoch}', fontsize=14, fontweight='bold')
                plt.tight_layout()
                plt.savefig(f'gan_epoch_{epoch}.png', dpi=150)
                plt.close()
    
    return d_losses, g_losses


<h1>Complete example</h1>
if __name__ == "__main__":
    print("="*70)
    print("GENERATIVE ADVERSARIAL NETWORK (GAN) TRAINING")
    print("="*70)
    
    # Setup
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nUsing device: {device}")
    
    # Hyperparameters
    latent_dim = 100
    batch_size = 128
    n_epochs = 50
    
    # Load data
    print("\nLoading MNIST dataset...")
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])  # Normalize to [-1, 1]
    ])
    
    dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # Create models
    generator = Generator(latent_dim=latent_dim).to(device)
    discriminator = Discriminator().to(device)
    
    print(f"\nGenerator parameters: {sum(p.numel() for p in generator.parameters()):,}")
    print(f"Discriminator parameters: {sum(p.numel() for p in discriminator.parameters()):,}")
    
    # Train
    d_losses, g_losses = train_gan(
        generator, discriminator, dataloader, 
        n_epochs, latent_dim, device
    )
    
    # Visualize training
    plt.figure(figsize=(10, 5))
    plt.plot(d_losses, label='Discriminator Loss')
    plt.plot(g_losses, label='Generator Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('GAN Training Progress')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig('gan_training_losses.png', dpi=300)
    plt.show()
    
    print("\n" + "="*70)
    print("TRAINING COMPLETE!")
    print("="*70)
    print("‚úì Generator can now create realistic images from random noise!")
</code></pre>

<strong>Key Advantages:</strong>
<li>‚úÖ Sharp, realistic images</li>
<li>‚úÖ High quality outputs</li>
<li>‚úÖ Versatile (images, video, audio)</li>

<strong>Limitations:</strong>
<li>‚ùå Training instability (mode collapse)</li>
<li>‚ùå Difficult to converge</li>
<li>‚ùå Limited control over outputs</li>

<strong>Famous GANs:</strong>
<li>StyleGAN (faces)</li>
<li>BigGAN (ImageNet)</li>
<li>Pix2Pix (image-to-image)</li>
<li>CycleGAN (unpaired translation)</li>

---

<h2>Slide 7: GAN Applications</h2>

<h3>Real-World Business Use Cases</h3>

<strong>1. Face Generation (StyleGAN)</strong>
<li>Create realistic human faces</li>
<li>Adjust age, gender, expression</li>
<li>Use: Avatars, game characters, privacy</li>

<strong>2. Image-to-Image Translation</strong>
<li>Pix2Pix: Sketch ‚Üí Photo</li>
<li>CycleGAN: Summer ‚Üí Winter, Horse ‚Üí Zebra</li>
<li>Use: Design automation, video effects</li>

<strong>3. Super-Resolution</strong>
<li>Enhance low-resolution images</li>
<li>Restore old photos</li>
<li>Use: Medical imaging, surveillance</li>

<strong>4. Deepfakes (Ethical Concerns)</strong>
<li>Face swapping in videos</li>
<li>Voice cloning</li>
<li>Use: Entertainment, BUT major ethical issues</li>

<strong>Business ROI Example:</strong>

<pre><code class="language-python"># Fashion Industry Use Case
traditional_photoshoot = {
    'model_fee': 2000,
    'photographer': 1500,
    'studio_rental': 1000,
    'post_processing': 500,
    'total_per_outfit': 5000,
    'time': '1 day'
}

gan_approach = {
    'model_3d_scan': 500,  # One-time
    'gan_generation': 10,  # Per outfit
    'retouching': 100,
    'total_per_outfit': 110,
    'time': '1 hour'
}

savings_per_outfit = traditional_photoshoot['total_per_outfit'] - gan_approach['total_per_outfit']
print(f"Savings: ${savings_per_outfit} per outfit (98% cost reduction)")
print(f"For 100 outfits: ${savings_per_outfit * 100:,} saved")
</code></pre>

---

<h2>Slide 8: Mode Collapse in GANs</h2>

<h3>A Common Training Problem</h3>

<strong>What is Mode Collapse?</strong>

Generator learns to produce only a few types of outputs (modes) instead of the full diversity of the data distribution.

<strong>Example:</strong>
<li>Training on faces dataset with thousands of unique people</li>
<li>Generator only produces 10-20 different faces</li>
<li>High quality but low diversity</li>

<strong>Why It Happens:</strong>
1. Generator finds a few outputs that fool discriminator
2. Keeps generating those "winning" outputs
3. Doesn't explore other possibilities
4. Gets stuck in local optimum

<strong>Visual Example:</strong>
<pre><code>Desired:  üòÄ üòÉ üòÑ üòÅ üòÜ üòÖ üòÇ ü§£ üòä üòá ... (1000s of variations)
Reality:  üòÄ üòÄ üòÄ üòÉ üòÉ üòÉ üòÑ üòÑ üòÑ  (only 3 variations)
</code></pre>

<strong>Solutions:</strong>

<strong>1. Minibatch Discrimination</strong>
<li>Discriminator compares images within a batch</li>
<li>Penalizes if all images are similar</li>
<li>Encourages diversity</li>

<strong>2. Unrolled GANs</strong>
<li>Generator looks ahead several discriminator update steps</li>
<li>Prevents short-term exploitation</li>

<strong>3. Wasserstein GAN (WGAN)</strong>
<li>Different loss function (Earth Mover's Distance)</li>
<li>More stable training</li>
<li>Meaningful loss curves</li>

<strong>4. Conditional GANs</strong>
<li>Add class labels as input</li>
<li>Force generator to produce specific types</li>
<li>Better control and diversity</li>

<strong>Business Impact:</strong>
<li>Mode collapse ‚Üí Limited usefulness</li>
<li>Stable training ‚Üí Reliable deployment</li>
<li>Diversity ‚Üí Better creative applications</li>

---

<h2>Slide 9: Conditional GANs (cGANs)</h2>

<h3>Adding Control to Generation</h3>

<strong>Problem with Standard GANs:</strong>
<li>Can't control what gets generated</li>
<li>Random noise ‚Üí random output</li>
<li>No way to specify "generate a 7" or "make it blue"</li>

<strong>Solution: Conditional GANs</strong>

Add additional information (labels, attributes) to both generator and discriminator:

<strong>Architecture:</strong>
<pre><code>Generator:  (Noise + Label) ‚Üí Image
Discriminator: (Image + Label) ‚Üí Real/Fake
</code></pre>

<strong>Implementation Example:</strong>

<pre><code class="language-python">class ConditionalGenerator(nn.Module):
    """Generator that takes class label as additional input"""
    def __init__(self, latent_dim=100, n_classes=10, img_channels=1):
        super().__init__()
        
        # Embedding for class labels
        self.label_emb = nn.Embedding(n_classes, latent_dim)
        
        # Generator (now takes latent_dim * 2)
        self.model = nn.Sequential(
            nn.Linear(latent_dim * 2, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 28 * 28),
            nn.Tanh()
        )
    
    def forward(self, noise, labels):
        # Concatenate noise and embedded labels
        label_embedding = self.label_emb(labels)
        combined_input = torch.cat([noise, label_embedding], dim=1)
        
        # Generate image
        img = self.model(combined_input)
        img = img.view(-1, 1, 28, 28)
        return img


<h1>Usage: Generate specific digit</h1>
generator = ConditionalGenerator()
noise = torch.randn(1, 100)
label = torch.tensor([7])  # Generate a "7"
generated_seven = generator(noise, label)
</code></pre>

<strong>Applications:</strong>

<strong>1. Text-to-Image (like DALL-E)</strong>
<li>Text prompt as condition</li>
<li>Generate images matching description</li>
<li>Revolutionary for creative industries</li>

<strong>2. Image-to-Image Translation</strong>
<li>Pix2Pix: Sketch + "make realistic" ‚Üí Photo</li>
<li>Edges + "add colors" ‚Üí Colored image</li>

<strong>3. Style Transfer</strong>
<li>Content image + Style reference ‚Üí Stylized image</li>
<li>Preserve content, change artistic style</li>

<strong>4. Face Editing</strong>
<li>Face + Attributes (add glasses, smile) ‚Üí Modified face</li>
<li>Precise control over generation</li>

<strong>Business Use Case:</strong>
<pre><code>E-commerce Product Images:
<li>Input: Product sketch + "professional studio lighting"</li>
<li>Output: High-quality product photo</li>
<li>Saves: $100-500 per product photoshoot</li>
<li>ROI: 95% cost reduction for 1000+ products</li>
</code></pre>

---

<h2>Slide 10: Diffusion Models Introduction</h2>

<h3>The New State-of-the-Art</h3>

<strong>What Changed Everything: Diffusion Models (2020+)</strong>

Diffusion models are now the leading approach for image generation, powering:
<li>DALL-E 2 & 3</li>
<li>Stable Diffusion</li>
<li>Midjourney</li>
<li>Imagen (Google)</li>

<strong>The Big Idea:</strong>

<strong>Forward Process (Adding Noise):</strong>
<pre><code>Clean Image ‚Üí Add noise ‚Üí Add more noise ‚Üí ... ‚Üí Pure noise
    ‚Üì            ‚Üì              ‚Üì                      ‚Üì
  Step 0      Step 1        Step 2              Step 1000
</code></pre>

<strong>Reverse Process (Denoising):</strong>
<pre><code>Pure noise ‚Üí Denoise ‚Üí Denoise more ‚Üí ... ‚Üí Clean Image
    ‚Üì          ‚Üì            ‚Üì                    ‚Üì
 Step 1000  Step 999    Step 998            Step 0
</code></pre>

<strong>Key Insight:</strong> If we learn to reverse the noise-adding process, we can generate images from pure noise!

<strong>Why Diffusion Models Won:</strong>

‚úÖ <strong>Better Quality:</strong> More realistic than GANs
‚úÖ <strong>More Stable:</strong> Easier to train than GANs
‚úÖ <strong>Better Control:</strong> Text conditioning works excellently
‚úÖ <strong>Scalable:</strong> Works well with massive models
‚úÖ <strong>Diverse:</strong> No mode collapse issues

<strong>The Numbers:</strong>
<li>DALL-E 2: 3.5B parameters</li>
<li>Stable Diffusion: 860M parameters</li>
<li>Training cost: $50M+ for DALL-E 2</li>
<li>Business impact: $10B+ market by 2028</li>

<strong>Simple Analogy:</strong>

Think of it like restoring an old, damaged photo:
1. Start with extremely noisy image (like static on TV)
2. AI gradually removes noise, revealing structure
3. Each step makes image slightly clearer
4. After 1000 steps: Perfect, new image

<strong>Coming up:</strong> Full implementation in Slide 11-13!

---

<strong>End of Batch 1 (Slides 1-10)</strong>

<em>Continue to Batch 2 for Diffusion Model Implementation and Audio Generation</em>

    </div>
</body>
</html>